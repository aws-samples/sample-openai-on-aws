{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "openai-compatible-frameworks",
   "metadata": {},
   "source": [
    "# Using Amazon Bedrock's OpenAI Compatible API with OSS Agentic Frameworks\n",
    "\n",
    "This notebook demonstrates how to leverage the OpenAI compatible API on Amazon Bedrock with popular open source frameworks including Strands Agents, LangChain/LangGraph, and CrewAI. We'll explore how these frameworks can seamlessly work with Bedrock's OpenAI-compatible endpoint.\n",
    "## What You'll Learn\n",
    "\n",
    "- Setting up OpenAI compatible API with Amazon Bedrock\n",
    "- Integrating Strands Agents with Bedrock\n",
    "- Using LangChain/LangGraph with Bedrock's OpenAI endpoint\n",
    "- Implementing CrewAI multi-agent workflows on Bedrock\n",
    "- Best practices for framework integration\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure you have:\n",
    "- Access to Amazon Bedrock with OpenAI models enabled\n",
    "- AWS credentials configured\n",
    "- Python environment with required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-environment",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install the required packages and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U boto3 openai langchain langgraph crewai strands-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "\n",
    "# Strands imports\n",
    "from strands import Agent as StrandsAgent\n",
    "from strands.models.bedrock import BedrockModel\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedrock-config",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Configuration\n",
    "\n",
    "Configure the environment to use Amazon Bedrock's OpenAI compatible endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-bedrock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "MODEL_ID = \"gpt-oss-120b\"  # or \"gpt-oss-20b\" for faster inference\n",
    "\n",
    "# Set region to use for Bedrock\n",
    "target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-west-2\"))\n",
    "\n",
    "# Set environment variables for OpenAI SDK to point to Bedrock\n",
    "os.environ[\"OPENAI_BASE_URL\"] = f'https://bedrock-runtime.{target_region}.amazonaws.com/openai/v1'\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<insert-your-bedrock-api-key>\"\n",
    "\n",
    "# Initialize OpenAI client (pointing to Bedrock)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "print(f\"âœ… Using model: {MODEL_ID}\")\n",
    "print(\"âœ… OpenAI client configured for Bedrock\")\n",
    "print(\"âœ… Bedrock client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strands-agents",
   "metadata": {},
   "source": [
    "## 1. Strands Agents with Amazon Bedrock\n",
    "\n",
    "Strands is AWS's open-source agent framework that provides single-file agents, built-in multi-agent hand-offs, and first-class Bedrock support.\n",
    "\n",
    "### Model setup\n",
    "\n",
    "We start by initializing the model wrapper class `BedrockModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strands-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock model for Strands\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=MODEL_ID,\n",
    "    region=\"us-west-2\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Strands Bedrock model initialized: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizer-Triager Agent\n",
    "\n",
    "We build a simple agentic system consisting of two agents that summarize and then triage customer support tickets. This demonstrates OpenAI OSS model's elevated capabilities for building agentic applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strands-simple-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = StrandsAgent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=(\n",
    "       \"You are SupportSummarizer. \"\n",
    "        \"For each ticket in the list you receive:\\n\"\n",
    "        \"  * Produce JSON with keys id, summary (â‰¤25 words), severity (low|medium|high)\\n\"\n",
    "        \"Return **only** a JSON array.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "triager = Agent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=(\n",
    "        \"You are TriageBot. You get a JSON array of tickets that already \"\n",
    "        \"contain severity. For every item output:\\n\"\n",
    "        \"  * id (same as input)\\n\"\n",
    "        \"  * route = 'escalate' if severity is high, else 'backlog'\\n\"\n",
    "        \"Respond with **only** a JSON array.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "tickets = [\n",
    "    {\"id\": \"1\", \"text\": \"User cannot log in after resetting password â€“ URGENT, blocks payroll.\"},\n",
    "    {\"id\": \"2\", \"text\": \"Minor typo in footer on marketing site.\"},\n",
    "]\n",
    "\n",
    "# summarise\n",
    "json_summaries = summarizer([t[\"text\"] for t in tickets]).content   \n",
    "\n",
    "# triage\n",
    "routes = triager(json_summaries).content\n",
    "\n",
    "print(\"Summaries:\", json_summaries)\n",
    "print(\"Routes   :\", routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer-Reporter Agent\n",
    "\n",
    "OpenAI OSS models are also optimized for reasoning in agentic applications. To demonstrate this, we set up a Strands Multi-Agent system for analyzing data and creating structured reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strands-multi-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_agent = StrandsAgent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=(\n",
    "        \"You are a DataAnalyzer. Analyze the given data and extract key insights. \"\n",
    "        \"Return your analysis in JSON format with keys: 'insights', 'trends', 'recommendations'.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "reporter_agent = StrandsAgent(\n",
    "    model=bedrock_model,\n",
    "    system_prompt=(\n",
    "        \"You are a ReportWriter. Take the analysis results and create a clear, \"\n",
    "        \"professional report. Focus on actionable insights and next steps.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sample data for analysis\n",
    "sample_data = \"\"\"\n",
    "Monthly Performance Data:\n",
    "- January: 85% customer satisfaction, 1200 users\n",
    "- February: 87% customer satisfaction, 1350 users  \n",
    "- March: 89% customer satisfaction, 1500 users\n",
    "- April: 91% customer satisfaction, 1650 users\n",
    "- May: 93% customer satisfaction, 1800 users\n",
    "\"\"\"\n",
    "\n",
    "# Multi-agent workflow\n",
    "print(\"ðŸ“Š Running Multi-Agent Analysis...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Analyze data\n",
    "analysis = analyzer_agent(sample_data)\n",
    "print(\"ðŸ“ˆ Analysis Results:\")\n",
    "print(analysis.content)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Step 2: Generate report\n",
    "report = reporter_agent(analysis.content)\n",
    "print(\"ðŸ“‹ Final Report:\")\n",
    "print(report.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langchain-integration",
   "metadata": {},
   "source": [
    "## 2. LangChain and LangGraph with Amazon Bedrock\n",
    "\n",
    "### Using LangChain's OpenAI wrapper classes natively with Amazon Bedrock\n",
    "\n",
    "LangChain provides a comprehensive framework for building LLM applications. We'll explore how to use LangChain's popular OpenAI wrapper classes with Bedrock's OpenAI compatible endpoint. \n",
    "\n",
    "**Note:** The environment variables `OPENAI_BASE_URL` and `OPENAI_API_KEY` set in the Amazon Bedrock Configuration section above will redirect traffic to Amazon Bedrock's OpenAI compatible endpoint and handle authentication via the Amazon Bedrock API key. Alternatively, these settings can be specified directly in the `ChatOpenAI` wrapper class through the parameters `base_url` and `api_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langchain-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain with Bedrock\n",
    "langchain_llm = ChatOpenAI(\n",
    "    model=MODEL_ID,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(f\"âœ… LangChain LLM initialized with model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langchain-simple-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LangChain Chain\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding assistant. Provide clear, concise explanations.\"),\n",
    "    (\"human\", \"Explain the concept of {topic} in programming.\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=langchain_llm, prompt=prompt_template)\n",
    "\n",
    "# Test the chain\n",
    "response = chain.run(topic=\"dependency injection\")\n",
    "\n",
    "print(\"ðŸ’» LangChain Chain Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph-workflow",
   "metadata": {},
   "source": [
    "### Building a LangGraph Workflow with LangChain's OpenAI Wrapper Classes and Amazon Bedrock\n",
    "\n",
    "LangGraph enables building complex, stateful workflows with LLMs. Let's create a multi-step workflow utilizing LangChain's OpenAI wrapper classes as demonstrated above. Our sample workflow will research a topic, analyze the results, and create a concise summary.\n",
    "\n",
    "Behind the scenes, this implementation utilizes Bedrock's OpenAI compatible endpoint.\n",
    "\n",
    "\n",
    "**Note:** The environment variables `OPENAI_BASE_URL` and `OPENAI_API_KEY` set in the Amazon Bedrock Configuration section above will redirect traffic to Amazon Bedrock's OpenAI compatible endpoint and handle the authentication via Amazon Bedrock API key. Alternatively, these settings can be specified directly in the `ChatOpenAI` wrapper class through the parameters `base_url` and `api_key`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langgraph-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state structure\n",
    "class WorkflowState(TypedDict):\n",
    "    query: str\n",
    "    research: str\n",
    "    analysis: str\n",
    "    summary: str\n",
    "\n",
    "# Initialize LangGraph LLM\n",
    "langgraph_llm = ChatOpenAI(\n",
    "    model=MODEL_ID,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"âœ… LangGraph setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langgraph-workflow-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define workflow functions\n",
    "def research_topic(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Research the given topic\"\"\"\n",
    "    prompt = f\"Research the following topic: {state['query']}. Provide comprehensive information.\"\n",
    "    response = langgraph_llm.invoke([HumanMessage(content=prompt)])\n",
    "    state['research'] = response.content\n",
    "    return state\n",
    "\n",
    "def analyze_research(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Analyze the research findings\"\"\"\n",
    "    prompt = f\"Analyze this research: {state['research']}. Identify key insights and patterns.\"\n",
    "    response = langgraph_llm.invoke([HumanMessage(content=prompt)])\n",
    "    state['analysis'] = response.content\n",
    "    return state\n",
    "\n",
    "def create_summary(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Create a final summary\"\"\"\n",
    "    prompt = f\"Create a concise summary based on: Research: {state['research']}, Analysis: {state['analysis']}\"\n",
    "    response = langgraph_llm.invoke([HumanMessage(content=prompt)])\n",
    "    state['summary'] = response.content\n",
    "    return state\n",
    "\n",
    "print(\"âœ… Workflow functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langgraph-workflow-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workflow graph\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"research\", research_topic)\n",
    "workflow.add_node(\"analyze\", analyze_research)\n",
    "workflow.add_node(\"summarize\", create_summary)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"research\", \"analyze\")\n",
    "workflow.add_edge(\"analyze\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"âœ… LangGraph workflow compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langgraph-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the workflow\n",
    "initial_state = {\n",
    "    \"query\": \"The impact of artificial intelligence on healthcare\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ”„ Executing LangGraph Workflow...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "print(\"ðŸ“‹ Final Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['summary'])\n",
    "\n",
    "print(\"\\nðŸ“Š Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crewai-integration",
   "metadata": {},
   "source": [
    "## 3. CrewAI with Amazon Bedrock\n",
    "\n",
    "CrewAI enables building multi-agent systems where agents can collaborate on complex tasks. Let's create a multi-agent crew utilizing CrewAI's `LLM` wrapper class. Our sample crew will research a topic, write content based on the results, and perform a final quality review.\n",
    "\n",
    "Behind the scenes, this implementation utilizes Bedrock's OpenAI compatible endpoint.\n",
    "\n",
    "**Note:** The environment variables `OPENAI_BASE_URL` and `OPENAI_API_KEY` set in the Amazon Bedrock Configuration section above will redirect traffic to Amazon Bedrock's OpenAI compatible endpoint and handle the authentication via Amazon Bedrock API key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crewai-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CrewAI to use Bedrock\n",
    "from crewai import Agent, Task, Crew, LLM\n",
    "\n",
    "crewai_llm = LLM(\n",
    "    model=f'openai/MODEL_ID', \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(f\"âœ… CrewAI LLM initialized with model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crewai-agents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialized agents\n",
    "researcher = Agent(\n",
    "    role='Research Analyst',\n",
    "    goal='Conduct thorough research on given topics',\n",
    "    backstory='Expert at gathering and analyzing information from various sources',\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=crewai_llm\n",
    ")\n",
    "\n",
    "writer = Agent(\n",
    "    role='Content Writer',\n",
    "    goal='Create engaging and informative content based on research',\n",
    "    backstory='Skilled writer with expertise in creating compelling narratives',\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=crewai_llm\n",
    ")\n",
    "\n",
    "reviewer = Agent(\n",
    "    role='Quality Reviewer',\n",
    "    goal='Ensure content quality, accuracy, and completeness',\n",
    "    backstory='Experienced editor with keen eye for detail and quality assurance',\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=crewai_llm\n",
    ")\n",
    "\n",
    "print(\"âœ… CrewAI agents defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crewai-tasks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tasks for the crew\n",
    "research_task = Task(\n",
    "    description=(\n",
    "        \"Research the topic: 'Sustainable AI Development Practices'. \"\n",
    "        \"Focus on environmental impact, ethical considerations, and best practices. \"\n",
    "        \"Provide comprehensive findings with key insights.\"\n",
    "    ),\n",
    "    agent=researcher,\n",
    "    expected_output=\"Detailed research findings with key insights and data points\"\n",
    ")\n",
    "\n",
    "writing_task = Task(\n",
    "    description=(\n",
    "        \"Based on the research findings, create a comprehensive article about \"\n",
    "        \"Sustainable AI Development Practices. The article should be engaging, \"\n",
    "        \"informative, and suitable for a technical audience.\"\n",
    "    ),\n",
    "    agent=writer,\n",
    "    expected_output=\"A well-structured article with introduction, main sections, and conclusion\"\n",
    ")\n",
    "\n",
    "review_task = Task(\n",
    "    description=(\n",
    "        \"Review the article for accuracy, completeness, and quality. \"\n",
    "        \"Ensure it meets professional standards and provides valuable insights. \"\n",
    "        \"Suggest improvements if needed.\"\n",
    "    ),\n",
    "    agent=reviewer,\n",
    "    expected_output=\"Quality review with feedback and suggestions for improvement\"\n",
    ")\n",
    "\n",
    "print(\"âœ… CrewAI tasks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crewai-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and execute the crew\n",
    "crew = Crew(\n",
    "    agents=[researcher, writer, reviewer],\n",
    "    tasks=[research_task, writing_task, review_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Executing CrewAI Workflow...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"ðŸ“„ Final Article:\")\n",
    "print(\"=\" * 50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## What We've Accomplished\n",
    "\n",
    "You've successfully integrated four major open source agentic frameworks with Amazon Bedrock's OpenAI compatible API:\n",
    "\n",
    "- âœ… **Strands Agents**: AWS-native agent framework with multi-agent workflows\n",
    "- âœ… **LangChain**: Comprehensive LLM application development with chains and tools  \n",
    "- âœ… **LangGraph**: Stateful workflow orchestration for complex processes\n",
    "- âœ… **CrewAI**: Multi-agent collaboration for specialized task decomposition\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- ðŸŽ¯ **Unified API**: All frameworks work seamlessly with Bedrock's OpenAI endpoint\n",
    "- ðŸš€ **Performance**: Each framework optimized for different use cases\n",
    "- ðŸ”§ **Developer Experience**: Familiar LLM wrappers integrate with Amazon Bedrock, enhancing developer experience and reducing migration efforts\n",
    "- ðŸ’° **Cost Efficiency**: Competitive pricing with enterprise reliability\n",
    "\n",
    "\n",
    "The OpenAI compatible API on Amazon Bedrock provides unprecedented flexibility for building sophisticated AI applications while maintaining familiar patterns and ecosystems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
