{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896fda04",
   "metadata": {},
   "source": [
    "# OpenAI OSS Model Getting Started Guide on Amazon Bedrock\n",
    "\n",
    "This notebook provides a comprehensive introduction to using gpt-oss-120b & gpt-oss-20b on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for both model variants. \n",
    "\n",
    "## Model Variants\n",
    "\n",
    "### GPT-OSS-120b\n",
    "\n",
    "Parameters: 120 billion\n",
    "\n",
    "Use Cases: Complex reasoning tasks, agentic use cases\n",
    "\n",
    "### GPT-OSS-20b\n",
    "\n",
    "Parameters: 20 billion\n",
    "\n",
    "Use Cases: Faster inference, cost-effective deployments, simpler tasks\n",
    "\n",
    "## Core Capabilities\n",
    "\n",
    "Both OpenAI model variants share the following characteristics:\n",
    "\n",
    "**Input/Output:** Text-in, text-out \n",
    "\n",
    "**Context Window:** 128,000 tokens  \n",
    "\n",
    "**Model Type:** Advanced reasoning models\n",
    "\n",
    "**Region:** us-west-2\n",
    "\n",
    "**Tool Calling:** ✅ Supported\n",
    "\n",
    "**Bedrock Guardrails** ✅ Supported\n",
    "\n",
    "**Converse API** ✅ Supported\n",
    "\n",
    "**OpenAI Chat Completions API** ✅ Supported\n",
    "\n",
    "**Web Search:** ❌ Not available at this time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c69a6",
   "metadata": {},
   "source": [
    "## What You'll Learn in this getting started guide\n",
    "\n",
    "- Options to use Amazon Bedrock for GPT-OSS inference, including:    \n",
    "    - Using the OpenAI SDK with Amazon Bedrock\n",
    "    - Using Amazon Bedrock's InvokeModel API\n",
    "    - Using Amazon Bedrock's Converse API\n",
    "- Understanding request parameters and response structures\n",
    "- Choosing between Large and Small models for your use case\n",
    "- Implementing tool calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe38fc",
   "metadata": {},
   "source": [
    "## Model Access on Amazon Bedrock\n",
    "\n",
    "Ensure you have the correct IAM permission in order to access OpenAI's models on Amazon Bedrock. In order to ensure you have model access, follow these steps: \n",
    "\n",
    "- Go to Amazon Bedrock --> model access\n",
    "- Click Modify model access\n",
    "- Scroll to OpenAI\n",
    "- Click the checkbox next to the models you would like access to\n",
    "- Click next & accept any EULAs\n",
    "- Click submit\n",
    "\n",
    "## IAM Permissions\n",
    "\n",
    "To use Bedrock models, your AWS credentials need the following permissions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719740df",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:InvokeModel\",\n",
    "        \"bedrock:InvokeModelWithResponseStream\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee373ed",
   "metadata": {},
   "source": [
    "## Step 1: Environment Configuration\n",
    "\n",
    "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
    "\n",
    "### Required Imports:\n",
    "- `os` → For environment variables\n",
    "- `boto3` → For native Bedrock API interactions  \n",
    "- `json` → For JSON serialization/deserialization\n",
    "- `datetime` → For timestamp tracking and performance measurements\n",
    "- `openai` → For OpenAI SDK compatibility with Bedrock\n",
    "- `strands` → For Amazon Strands agent framework\n",
    "- `IPython.display` → For enhanced output formatting and streaming demonstrations\n",
    "\n",
    "### Environment Variables:\n",
    "We set two environment variables to redirect the OpenAI SDK:\n",
    "- `AWS_BEARER_TOKEN_BEDROCK` → Your Bedrock API key  \n",
    "- `OPENAI_BASE_URL` → Bedrock's OpenAI-compatible endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -U install boto3 openai strands-agents ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906523c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from strands import Agent\n",
    "from strands.models.openai import OpenAIModel\n",
    "from strands.models import BedrockModel\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display, display_markdown, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882e70b",
   "metadata": {},
   "source": [
    "### Model IDs\n",
    "\n",
    "- **openai.gpt-oss-120b-1:0**\n",
    "- **openai.gpt-oss-20b-1:0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration - Change this to your desired model\n",
    "MODEL_ID = \"gpt-oss-120b\"  \n",
    "\n",
    "print(f\"✅ Using model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables to point to Bedrock\n",
    "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"<insert your bedrock API key>\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
    "\n",
    "print(\"✅ Environment configured for Bedrock!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6caae",
   "metadata": {},
   "source": [
    "## Step 2: Inference with Amazon Bedrock\n",
    "\n",
    "### Option 1: OpenAI SDK\n",
    "\n",
    "#### Import and Initialize OpenAI Client\n",
    "\n",
    "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
    "\n",
    "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cec1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both clients\n",
    "client = OpenAI()  # For chat completions API\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')  \n",
    "\n",
    "print(\"✅ OpenAI client initialized (pointing to Bedrock)\")\n",
    "print(f\"✅ Bedrock client initialized in region: {bedrock_client.meta.region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12a7c2",
   "metadata": {},
   "source": [
    "#### Make API Calls \n",
    "\n",
    "The API call structure is identical to OpenAI:\n",
    "- Same `messages` format with `role` and `content`\n",
    "- Same `model` parameter (but uses Bedrock model IDs)  \n",
    "- Same `stream` parameter for real-time responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss-120b\",                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    max_tokens=1000,\n",
    "    stream=True                         \n",
    ")\n",
    "\n",
    "print(\"✅ API request created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a09f4",
   "metadata": {},
   "source": [
    "#### Process Streaming Response\n",
    "\n",
    "Handle the response exactly like you would with OpenAI. Each `item` in the response is a chunk of the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20620d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in response:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0c28e",
   "metadata": {},
   "source": [
    "#### What's Happening Behind the Scenes?\n",
    "\n",
    "When you use the OpenAI SDK with Bedrock, your requests are automatically translated to Bedrock's native `InvokeModel` API.\n",
    "\n",
    "#### Request Translation\n",
    "- **OpenAI SDK Request** → **Bedrock InvokeModel** \n",
    "- The request body structure remains the same\n",
    "- But there are some key differences in how parameters are handled:\n",
    "\n",
    "| Parameter | OpenAI SDK | Bedrock InvokeModel |\n",
    "|-----------|------------|-------------------|\n",
    "| **Model ID** | In request body | Part of the URL path |\n",
    "| **Streaming** | `stream=True/False` | Different API endpoints:<br/>• `InvokeModel` (non-streaming)<br/>• `InvokeModelWithResponseStream` (streaming) |\n",
    "| **Request Body** | Full chat completions format | Same format, but `model` and `stream` are optional |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25d8ae",
   "metadata": {},
   "source": [
    "#### Function Calling with OpenAI SDK\n",
    "\n",
    "The OpenAI SDK interface on Bedrock supports function calling, allowing the model to invoke external functions. Let's demonstrate this with a weather lookup function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "    \"\"\"\n",
    "    Get current weather for a given location.\n",
    "    This is a mock function that returns sample weather data.\n",
    "    \n",
    "    Args:\n",
    "        location (str): City and country, e.g. \"Paris, France\"\n",
    "        \n",
    "    Returns:\n",
    "        dict: Weather information\n",
    "    \"\"\"\n",
    "    # Mock weather data - in a real application, you'd call a weather API\n",
    "    weather_data = {\n",
    "        \"Paris, France\": {\"temperature\": \"22°C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
    "        \"New York, USA\": {\"temperature\": \"18°C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
    "        \"Tokyo, Japan\": {\"temperature\": \"25°C\", \"condition\": \"Rainy\", \"humidity\": \"80%\"},\n",
    "        \"London, UK\": {\"temperature\": \"15°C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"},\n",
    "        \"Sydney, Australia\": {\"temperature\": \"28°C\", \"condition\": \"Clear\", \"humidity\": \"55%\"}\n",
    "    }\n",
    "    \n",
    "    return weather_data.get(location, {\n",
    "        \"temperature\": \"20°C\", \n",
    "        \"condition\": \"Data not available\", \n",
    "        \"humidity\": \"50%\"\n",
    "    })\n",
    "\n",
    "# Define the function schema for OpenAI SDK\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature and weather conditions for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country, e.g. 'Paris, France'\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "print(\"✅ Weather function and tools configuration ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_functions(client, model, messages, tools, max_iterations=3):\n",
    "    \"\"\"\n",
    "    Chat with function calling support using OpenAI SDK format.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        model: Model ID to use\n",
    "        messages: List of conversation messages\n",
    "        tools: List of available tools/functions\n",
    "        max_iterations: Maximum number of function call iterations\n",
    "        \n",
    "    Returns:\n",
    "        Final assistant message\n",
    "    \"\"\"\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"🔄 Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Make request with tools\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        # Check if the model wants to call functions\n",
    "        if assistant_message.tool_calls:\n",
    "            print(f\"🔧 Model requested {len(assistant_message.tool_calls)} function call(s)\")\n",
    "            \n",
    "            # Process each function call\n",
    "            for tool_call in assistant_message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                print(f\"🔧 Calling function: {function_name}\")\n",
    "                print(f\"🔧 Arguments: {function_args}\")\n",
    "                \n",
    "                # Call the actual function\n",
    "                if function_name == \"get_weather\":\n",
    "                    function_result = get_weather(function_args[\"location\"])\n",
    "                    print(f\"🔧 Function result: {function_result}\")\n",
    "                else:\n",
    "                    function_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "                \n",
    "                # Add function result to conversation\n",
    "                function_message = {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": json.dumps(function_result)\n",
    "                }\n",
    "                messages.append(function_message)\n",
    "                \n",
    "        else:\n",
    "            # No more function calls, return final response\n",
    "            print(\"✅ No function calls requested, conversation complete\")\n",
    "            return assistant_message\n",
    "    \n",
    "    print(\"⚠️ Maximum iterations reached\")\n",
    "    return assistant_message\n",
    "\n",
    "print(\"✅ Function calling handler ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1edafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calling with various weather queries\n",
    "weather_questions = [\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    \"Can you tell me the temperature in Tokyo?\",\n",
    "    \"How's the weather in Sydney, Australia?\",\n",
    "    \"What are the conditions like in New York?\"\n",
    "]\n",
    "\n",
    "print(\"🌤️ Testing Function Calling with OpenAI SDK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(weather_questions, 1):\n",
    "    print(f\"\\n📝 Test {i}: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create conversation messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. Use the get_weather function to provide accurate weather information.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # Call the function calling handler\n",
    "        final_response = chat_with_functions(\n",
    "            client=client,\n",
    "            model=MODEL_ID,  # Uses the configured model ID\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        )\n",
    "        \n",
    "        # Print the final response\n",
    "        print(\"🤖 Final response:\")\n",
    "        print(final_response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa75d4",
   "metadata": {},
   "source": [
    "#### What Just Happened with Function Calling?\n",
    "\n",
    "The function calling demonstration above shows the OpenAI SDK workflow:\n",
    "\n",
    "1. **Function Definition**: We defined a weather function with OpenAI SDK schema format\n",
    "2. **Initial Request**: The model receives a weather question and recognizes it needs weather data\n",
    "3. **Function Call**: The model generates a structured function call request with proper arguments\n",
    "4. **Function Execution**: Our handler extracts the arguments, calls the actual function, and gets results\n",
    "5. **Result Integration**: The function result is added back to the conversation context\n",
    "6. **Final Response**: The model uses the function result to provide a natural language answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31281329",
   "metadata": {},
   "source": [
    "### Option 2: Amazon Bedrock's InvokeModel API\n",
    "\n",
    "The Bedrock InvokeModel API is the foundational interface for interacting directly with any model hosted on Amazon Bedrock. It provides low-level, flexible access to model inference, allowing you to send input data and receive generated responses in a consistent way across all supported models.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Direct Access: Interact with any Bedrock model using a unified API endpoint.\n",
    "- Fine-Grained Control: Customize inference parameters and payloads for each request.\n",
    "- Streaming Support: Use `InvokeModelWithResponseStream` for real-time, token-by-token output.\n",
    "- Privacy: Amazon Bedrock does not store your input or output data—requests are used only for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a4612",
   "metadata": {},
   "source": [
    "#### Setup client\n",
    "\n",
    "First, we setup the Amazon Bedrock client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76062050",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = None\n",
    "\n",
    "if region is None:\n",
    "    target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "else:\n",
    "    target_region = \"us-west-2\"\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2a1ca",
   "metadata": {},
   "source": [
    "#### Inference with InvokeModel API\n",
    "\n",
    "Then we use the InvokeModel API to perform model inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5121a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    \"\"\"\n",
    "    Invokes Amazon bedrock model to run an inference\n",
    "    using the input provided in the request body.\n",
    "    \n",
    "    Args:\n",
    "        body (dict): The invokation body to send to bedrock\n",
    "        model_id (str): the model to query\n",
    "        accept (str): input accept type\n",
    "        content_type (str): content type\n",
    "    Returns:\n",
    "        Inference response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "            {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "            ]\n",
    "  \n",
    "body = {\n",
    "    \"prompt\": messages,\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_gen_len\": 1000,\n",
    "}\n",
    "\n",
    "modelId = \"gpt-oss-120b\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, modelId, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9681e",
   "metadata": {},
   "source": [
    "#### Streaming with InvokeModel API\n",
    "\n",
    "The InvokeModel API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "            {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "            ]\n",
    "\n",
    "body = {\n",
    "    \"prompt\": messages,\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_gen_len\": 1000,\n",
    "}\n",
    "\n",
    "modelId = \"gpt-oss-120b\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "chunk_count = 0\n",
    "time_to_first_token = None\n",
    "\n",
    "# Process the response stream\n",
    "stream = response.get(\"body\")\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # Print the response chunk\n",
    "            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "            # Pretty print JSON\n",
    "            # print(json.dumps(chunk_json, indent=2, ensure_ascii=False))\n",
    "            content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "            if content_block_delta:\n",
    "                if time_to_first_token is None:\n",
    "                    time_to_first_token = datetime.now() - start_time\n",
    "                    print(f\"Time to first token: {time_to_first_token}\")\n",
    "\n",
    "                chunk_count += 1\n",
    "                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "                # print(f\"{current_time} - \", end=\"\")\n",
    "                print(content_block_delta.get(\"delta\").get(\"text\"), end=\"\")\n",
    "    print(f\"Total chunks: {chunk_count}\")\n",
    "else:\n",
    "    print(\"No response stream received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab48e3",
   "metadata": {},
   "source": [
    "#### Tool Use with InvokeModel API\n",
    "\n",
    "The InvokeModel API also supports tool calling through the native Bedrock payload format. Let's demonstrate this with the same weather lookup function to compare approaches across all three APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_invoke(location):\n",
    "    \"\"\"\n",
    "    Get current weather for a given location.\n",
    "    Same function as used in other API examples for consistency.\n",
    "    \n",
    "    Args:\n",
    "        location (str): City and country, e.g. \"Paris, France\"\n",
    "        \n",
    "    Returns:\n",
    "        dict: Weather information\n",
    "    \"\"\"\n",
    "    # Same weather data as other examples\n",
    "    weather_data = {\n",
    "        \"Paris, France\": {\"temperature\": \"22°C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
    "        \"New York, USA\": {\"temperature\": \"18°C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
    "        \"Tokyo, Japan\": {\"temperature\": \"25°C\", \"condition\": \"Rainy\", \"humidity\": \"80%\"},\n",
    "        \"London, UK\": {\"temperature\": \"15°C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"},\n",
    "        \"Sydney, Australia\": {\"temperature\": \"28°C\", \"condition\": \"Clear\", \"humidity\": \"55%\"}\n",
    "    }\n",
    "    \n",
    "    return weather_data.get(location, {\n",
    "        \"temperature\": \"20°C\", \n",
    "        \"condition\": \"Data not available\", \n",
    "        \"humidity\": \"50%\"\n",
    "    })\n",
    "\n",
    "# Tool configuration for InvokeModel API\n",
    "invoke_model_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current temperature and weather conditions for a given location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City and country, e.g. 'Paris, France'\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✅ Weather function and InvokeModel API tool configuration ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6486e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model_with_tools(bedrock_client, model_id, messages, tools, max_iterations=3):\n",
    "    \"\"\"\n",
    "    Generate text using InvokeModel API with tool support.\n",
    "    \n",
    "    Args:\n",
    "        bedrock_client: Boto3 Bedrock runtime client\n",
    "        model_id: Model ID to use\n",
    "        messages: List of conversation messages  \n",
    "        tools: List of available tools\n",
    "        max_iterations: Maximum tool calling iterations\n",
    "        \n",
    "    Returns:\n",
    "        Final response from the model\n",
    "    \"\"\"\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"🔄 Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Prepare the request body for InvokeModel\n",
    "        body = {\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools,\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "        \n",
    "        # Make the InvokeModel request\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=json.dumps(body),\n",
    "            modelId=model_id,\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        assistant_message = response_body.get(\"choices\", [{}])[0].get(\"message\", {})\n",
    "        \n",
    "        # Add assistant message to conversation\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        # Check if model wants to call tools\n",
    "        tool_calls = assistant_message.get(\"tool_calls\", [])\n",
    "        if tool_calls:\n",
    "            print(f\"🔧 Model requested {len(tool_calls)} tool call(s)\")\n",
    "            \n",
    "            # Process each tool call\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call[\"function\"][\"name\"]\n",
    "                function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "                \n",
    "                print(f\"🔧 Calling function: {function_name}\")\n",
    "                print(f\"🔧 Arguments: {function_args}\")\n",
    "                \n",
    "                # Call the actual function\n",
    "                if function_name == \"get_weather\":\n",
    "                    function_result = get_weather_invoke(function_args[\"location\"])\n",
    "                    print(f\"🔧 Function result: {function_result}\")\n",
    "                else:\n",
    "                    function_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "                \n",
    "                # Add function result to conversation\n",
    "                function_message = {\n",
    "                    \"tool_call_id\": tool_call[\"id\"],\n",
    "                    \"role\": \"tool\", \n",
    "                    \"name\": function_name,\n",
    "                    \"content\": json.dumps(function_result)\n",
    "                }\n",
    "                messages.append(function_message)\n",
    "                \n",
    "        else:\n",
    "            # No tool calls, return final response\n",
    "            print(\"✅ No tool calls requested, conversation complete\")\n",
    "            return assistant_message\n",
    "    \n",
    "    print(\"⚠️ Maximum iterations reached\")\n",
    "    return assistant_message\n",
    "\n",
    "print(\"✅ InvokeModel tool calling handler ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14489174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the weather tool with InvokeModel API\n",
    "weather_questions = [\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    \"Can you tell me the temperature in Tokyo?\", \n",
    "    \"How's the weather in Sydney, Australia?\",\n",
    "    \"What are the conditions like in New York?\"\n",
    "]\n",
    "\n",
    "print(\"🌤️ Testing Weather Tool with InvokeModel API\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(weather_questions, 1):\n",
    "    print(f\"\\n📝 Test {i}: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create conversation messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. Use the get_weather function to provide accurate weather information.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # Call the InvokeModel tool handler\n",
    "        final_response = invoke_model_with_tools(\n",
    "            bedrock_client=bedrock_runtime,\n",
    "            model_id=\"gpt-oss-120b\",  # You can change to gpt-oss-20b for faster inference\n",
    "            messages=messages,\n",
    "            tools=invoke_model_tools\n",
    "        )\n",
    "        \n",
    "        # Print the final response\n",
    "        print(\"🤖 Final response:\")\n",
    "        print(final_response.get(\"content\", \"No content in response\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87338d",
   "metadata": {},
   "source": [
    "#### What Just Happened with InvokeModel Tool Use?\n",
    "\n",
    "The InvokeModel API tool use demonstration shows the raw, low-level approach:\n",
    "\n",
    "1. **Tool Definition**: We defined tools using OpenAI SDK compatible format in the request body\n",
    "2. **Request Construction**: We manually built the full request payload including messages and tools\n",
    "3. **API Call**: Direct InvokeModel call with JSON body containing all parameters\n",
    "4. **Response Parsing**: Manual parsing of the JSON response to extract assistant messages\n",
    "5. **Tool Detection**: Checking for `tool_calls` in the assistant message\n",
    "6. **Function Execution**: Extracting arguments, calling functions, and handling results\n",
    "7. **Conversation Management**: Manually appending messages to maintain conversation context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2d0d7",
   "metadata": {},
   "source": [
    "### Option 3: Amazon Bedrock's Converse API\n",
    "\n",
    "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes.\n",
    "\n",
    "Key Benefits:\n",
    "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
    "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
    "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
    "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Say this is a test\"}]\n",
    "        }\n",
    "    ],\n",
    "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0.2,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bae17f",
   "metadata": {},
   "source": [
    "#### Tool Use with Converse API\n",
    "\n",
    "The Converse API supports tool calling, allowing the model to invoke external functions. Let's demonstrate this with a weather lookup function that provides weather information for different cities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_converse(location):\n",
    "    \"\"\"\n",
    "    Get current weather for a given location.\n",
    "    This is the same mock function used across all API examples.\n",
    "    \n",
    "    Args:\n",
    "        location (str): City and country, e.g. \"Paris, France\"\n",
    "        \n",
    "    Returns:\n",
    "        dict: Weather information\n",
    "    \"\"\"\n",
    "    # Mock weather data - same as used in OpenAI SDK example\n",
    "    weather_data = {\n",
    "        \"Paris, France\": {\"temperature\": \"22°C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
    "        \"New York, USA\": {\"temperature\": \"18°C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
    "        \"Tokyo, Japan\": {\"temperature\": \"25°C\", \"condition\": \"Rainy\", \"humidity\": \"80%\"},\n",
    "        \"London, UK\": {\"temperature\": \"15°C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"},\n",
    "        \"Sydney, Australia\": {\"temperature\": \"28°C\", \"condition\": \"Clear\", \"humidity\": \"55%\"}\n",
    "    }\n",
    "    \n",
    "    return weather_data.get(location, {\n",
    "        \"temperature\": \"20°C\", \n",
    "        \"condition\": \"Data not available\", \n",
    "        \"humidity\": \"50%\"\n",
    "    })\n",
    "\n",
    "# Define the tool configuration for Converse API\n",
    "converse_tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current temperature and weather conditions for a given location.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"City and country, e.g. 'Paris, France'\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"location\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"✅ Weather function and Converse API tool configuration ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_tools(bedrock_client, model_id, tool_config, input_text):\n",
    "    \"\"\"\n",
    "    Generates text using the Converse API with tool support. \n",
    "    Handles tool use requests and sends results back to the model.\n",
    "    \n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client\n",
    "        model_id (str): The Amazon Bedrock model ID\n",
    "        tool_config (dict): The tool configuration\n",
    "        input_text (str): The input text\n",
    "        \n",
    "    Returns:\n",
    "        The final response from the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the initial message from the user input\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }]\n",
    "    \n",
    "    # Make the initial request to the model\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    \n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "    \n",
    "    if stop_reason == 'tool_use':\n",
    "        print(\"🔧 Model requested tool use...\")\n",
    "        \n",
    "        # Process tool use requests\n",
    "        tool_requests = response['output']['message']['content']\n",
    "        for tool_request in tool_requests:\n",
    "            if 'toolUse' in tool_request:\n",
    "                tool = tool_request['toolUse']\n",
    "                print(f\"🔧 Calling tool: {tool['name']} with ID: {tool['toolUseId']}\")\n",
    "                print(f\"🔧 Tool input: {tool['input']}\")\n",
    "                \n",
    "                if tool['name'] == 'get_weather':\n",
    "                    # Call our weather function\n",
    "                    try:\n",
    "                        result = get_weather_converse(\n",
    "                            location=tool['input']['location']\n",
    "                        )\n",
    "                        print(f\"🔧 Tool result: {result}\")\n",
    "                        \n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"json\": result}]\n",
    "                        }\n",
    "                    except Exception as err:\n",
    "                        print(f\"❌ Tool error: {err}\")\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"text\": f\"Error: {str(err)}\"}],\n",
    "                            \"status\": 'error'\n",
    "                        }\n",
    "                    \n",
    "                    # Create the tool result message\n",
    "                    tool_result_message = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"toolResult\": tool_result}]\n",
    "                    }\n",
    "                    messages.append(tool_result_message)\n",
    "        \n",
    "        # Send the tool result back to the model\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            toolConfig=tool_config\n",
    "        )\n",
    "        output_message = response['output']['message']\n",
    "    \n",
    "    # Return the final response\n",
    "    return output_message\n",
    "\n",
    "print(\"✅ Tool use function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the weather tool with various location queries\n",
    "test_questions = [\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    \"Can you tell me the temperature in Tokyo?\",\n",
    "    \"How's the weather in Sydney, Australia?\",\n",
    "    \"What are the conditions like in New York?\"\n",
    "]\n",
    "\n",
    "print(\"🌤️ Testing Weather Tool with Converse API\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n📝 Test {i}: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Use gpt-oss-120b model ID - update this to match your configured MODEL_ID\n",
    "        response = generate_text_with_tools(\n",
    "            bedrock_client, \n",
    "            \"gpt-oss-120b\",  # You can change this to gpt-oss-20b for faster inference\n",
    "            converse_tool_config, \n",
    "            question\n",
    "        )\n",
    "        \n",
    "        # Print the final response from the model\n",
    "        print(\"🤖 Model response:\")\n",
    "        for content in response['content']:\n",
    "            if 'text' in content:\n",
    "                print(content['text'])\n",
    "            else:\n",
    "                print(json.dumps(content, indent=2))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31df4e",
   "metadata": {},
   "source": [
    "#### What Just Happened with Converse Tool Use?\n",
    "\n",
    "The tool use demonstration above shows the complete flow:\n",
    "\n",
    "1. **Tool Definition**: We defined a weather tool with a clear schema describing its inputs and outputs\n",
    "2. **Model Request**: The model receives a weather question and recognizes it needs to use the weather tool\n",
    "3. **Tool Invocation**: The model generates a structured tool use request with the appropriate parameters  \n",
    "4. **Tool Execution**: Our code intercepts this request, calls the actual weather function, and gets the result\n",
    "5. **Result Return**: We send the tool result back to the model in the proper format\n",
    "6. **Final Response**: The model incorporates the tool result into a natural language response\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Accuracy**: Weather data comes from actual function calls, not model approximation\n",
    "- **Reliability**: Tools provide deterministic, structured results every time\n",
    "- **Extensibility**: You can add any custom tools your application needs (APIs, databases, etc.)\n",
    "\n",
    "This same pattern works for any tool - from simple weather lookups to complex API integrations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0257ea",
   "metadata": {},
   "source": [
    "## Tool Use API Comparison\n",
    "\n",
    "Now that you've seen the same weather function implemented across all three APIs, here's a side-by-side comparison:\n",
    "\n",
    "| Aspect | OpenAI SDK | InvokeModel API | Converse API |\n",
    "|--------|------------|-----------------|--------------|\n",
    "| **Ease of Use** | ⭐⭐⭐⭐⭐ Familiar | ⭐⭐⭐ Manual | ⭐⭐⭐⭐ Consistent |\n",
    "| **Tool Schema** | OpenAI `function` format | OpenAI `function` format | Bedrock `toolSpec` format |\n",
    "| **Response Handling** | SDK handles parsing | Manual JSON parsing | SDK handles parsing |\n",
    "| **Error Management** | Built-in error handling | Custom error handling | Built-in error handling |\n",
    "| **Performance** | SDK overhead | Direct API calls | Moderate overhead |\n",
    "| **Flexibility** | Limited to OpenAI patterns | Complete customization | Bedrock-specific features |\n",
    "| **Migration** | Easy from OpenAI | Full custom implementation | Learn new format |\n",
    "\n",
    "### When to Choose Each Approach:\n",
    "\n",
    "**OpenAI SDK**: \n",
    "- ✅ Migrating from OpenAI\n",
    "- ✅ Want familiar patterns\n",
    "- ✅ Quick prototyping\n",
    "- ❌ Need Bedrock-specific features\n",
    "\n",
    "**InvokeModel API**:\n",
    "- ✅ Maximum control needed\n",
    "- ✅ Custom request processing\n",
    "- ✅ Performance critical applications\n",
    "- ❌ Don't want to handle parsing manually\n",
    "\n",
    "**Converse API**:\n",
    "- ✅ Multi-model compatibility\n",
    "- ✅ Bedrock native features\n",
    "- ✅ Guardrails integration\n",
    "- ❌ Learning new API format\n",
    "\n",
    "All three approaches produce the same results - choose based on your specific needs and preferences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3c82e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully explored **three powerful ways** to interact with OpenAI's GPT-OSS models on Amazon Bedrock, including comprehensive tool use capabilities!\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "**1. OpenAI SDK Integration**\n",
    "- Set up the familiar OpenAI SDK to work seamlessly with AWS Bedrock\n",
    "- Leveraged existing OpenAI patterns while running on AWS infrastructure\n",
    "- Demonstrated streaming responses with real-time token generation\n",
    "- **Implemented function calling** using familiar OpenAI SDK patterns\n",
    "\n",
    "**2. Direct InvokeModel API Access**\n",
    "- Implemented low-level Bedrock InvokeModel API calls for maximum control\n",
    "- Built custom functions for both streaming and non-streaming inference\n",
    "- Measured performance metrics like time-to-first-token for streaming responses\n",
    "- Gained fine-grained control over model parameters and request formatting\n",
    "- **Built custom tool use handling** with manual request/response processing\n",
    "\n",
    "**3. Bedrock Converse API**\n",
    "- Explored the unified Converse API that works across all Bedrock models\n",
    "- Demonstrated consistent message-based interactions regardless of underlying model\n",
    "- Leveraged built-in support for system prompts and inference configuration\n",
    "- **Integrated tool calling** using Bedrock's native toolSpec format\n",
    "\n",
    "**4. Tool Use Comparison**\n",
    "- Implemented the **same weather function across all three APIs** for direct comparison\n",
    "- Learned the different schema formats and response handling approaches\n",
    "- Understood when to choose each API based on your specific needs\n",
    "\n",
    "### Key Benefits Achieved\n",
    "\n",
    "✅ **Flexibility**: Three different API approaches for different use cases  \n",
    "✅ **Performance**: Streaming support for improved user experience  \n",
    "✅ **Familiarity**: Use existing OpenAI SDK patterns with AWS infrastructure  \n",
    "✅ **Control**: Direct API access when you need fine-grained customization  \n",
    "✅ **Consistency**: Universal interface that works across all Bedrock models  \n",
    "✅ **Privacy**: AWS Bedrock doesn't store your data - only used for inference  \n",
    "✅ **Tool Integration**: Function calling capabilities across all three approaches\n",
    "✅ **Practical Comparison**: Side-by-side examples using the same function\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You're now equipped with comprehensive knowledge to choose the right API approach for your specific use case. Whether you need:\n",
    "- The **simplicity** of the OpenAI SDK\n",
    "- The **control** of InvokeModel \n",
    "- The **consistency** of Converse API\n",
    "- **Tool use capabilities** for external integrations\n",
    "\n",
    "You have all the tools and examples to build powerful AI applications with external function calling on AWS Bedrock!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c253ab2",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "\n",
    "Let's test reasoning with the Chat Completions API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt, temperature=0.02, max_tokens=1000):\n",
    "    \"\"\"Helper function to run prompts with chat completions\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID, # update your model id to either 120b or 20b\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"✅ Helper function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54e021",
   "metadata": {},
   "source": [
    "## Reasoning Use Case Examples\n",
    "\n",
    "Testing complex reasoning and problem-solving capabilities with the Chat Completions API.\n",
    "\n",
    "### Exponential Growth Problem\n",
    "\n",
    "Tests compound growth calculations and iterative reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4766a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Solve and verify.\n",
    "\n",
    "A factory makes widgets. Day 1 output is 500. Each day after, output increases by 8%.\n",
    "How many days until daily output first exceeds 1,000?\n",
    "\n",
    "Return:\n",
    "- Short reasoning outline (≤4 lines)\n",
    "- Final answer (integer days)\n",
    "- One-line self-check (plug back).\"\"\"\n",
    "\n",
    "print(run_prompt(prompt, max_tokens=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff220e",
   "metadata": {},
   "source": [
    "### Kinematics Problem\n",
    "\n",
    "Tests multi-phase motion analysis and distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"A car accelerates uniformly from rest to 27 m/s in 9 s, then coasts at constant speed for 60 s, then brakes uniformly to rest in 6 s.\n",
    "Compute total distance traveled.\n",
    "\n",
    "Return:\n",
    "- Piecewise distances\n",
    "- Sum\n",
    "- One-line unit/sanity check.\"\"\"\n",
    "\n",
    "print(run_prompt(prompt, max_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1800db7",
   "metadata": {},
   "source": [
    "### Scientific Problem Solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_prompt = \"\"\"A ball is thrown vertically upward from ground level with an initial velocity of 20 m/s.\n",
    "Calculate:\n",
    "1. Maximum height reached\n",
    "2. Time to reach maximum height  \n",
    "3. Total time in the air\n",
    "4. Velocity when it returns to ground level\n",
    "\n",
    "Use g = 9.8 m/s². Show your physics reasoning and formulas.\"\"\"\n",
    "\n",
    "print(\"Scientific Problem Solving:\")\n",
    "print(\"=\" * 50)\n",
    "print(run_prompt(science_prompt, max_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a19194",
   "metadata": {},
   "source": [
    "### System Design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b54d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Design Example  \n",
    "system_prompt = \"\"\"Design a URL shortener service like bit.ly for 100M URLs per day.\n",
    "Address:\n",
    "1. Database schema design\n",
    "2. Encoding/decoding algorithm choice\n",
    "3. Scalability considerations (caching, load balancing)\n",
    "4. One potential bottleneck and solution\n",
    "\n",
    "Keep response concise but show your reasoning.\"\"\"\n",
    "\n",
    "print(\"System Design Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(run_prompt(system_prompt, max_tokens=3000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
