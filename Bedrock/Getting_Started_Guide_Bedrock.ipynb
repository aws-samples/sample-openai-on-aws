{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896fda04",
   "metadata": {},
   "source": [
    "# OpenAI OSS Model Getting Started Guide on Amazon Bedrock\n",
    "\n",
    "This notebook provides a comprehensive introduction to using gpt-oss-120b & gpt-oss-20b on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for both model variants. \n",
    "\n",
    "## Model Variants\n",
    "\n",
    "### GPT-OSS-120b\n",
    "\n",
    "Parameters: 120 billion\n",
    "\n",
    "Use Cases: Complex reasoning tasks, agentic use cases\n",
    "\n",
    "### GPT-OSS-20b\n",
    "\n",
    "Parameters: 20 billion\n",
    "\n",
    "Use Cases: Faster inference, cost-effective deployments, simpler tasks\n",
    "\n",
    "## Core Capabilities\n",
    "\n",
    "Both OpenAI model variants share the following characteristics:\n",
    "\n",
    "**Input/Output:** Text-in, text-out \n",
    "\n",
    "**Context Window:** 128,000 tokens  \n",
    "\n",
    "**Model Type:** Advanced reasoning models\n",
    "\n",
    "**Region:** us-west-2\n",
    "\n",
    "**Tool Calling:** ✅ Supported\n",
    "\n",
    "**Bedrock Guardrails** ✅ Supported\n",
    "\n",
    "**Converse API** ✅ Supported\n",
    "\n",
    "**OpenAI Chat Completions API** ✅ Supported\n",
    "\n",
    "**Web Search:** ❌ Not available at this time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c69a6",
   "metadata": {},
   "source": [
    "## What You'll Learn in this getting started guide\n",
    "\n",
    "- Setting up the OpenAI SDK to work with Amazon Bedrock\n",
    "- Understanding request parameters and response structures\n",
    "- Choosing between Large and Small models for your use case\n",
    "- Implementing tool calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe38fc",
   "metadata": {},
   "source": [
    "## Model Access on Amazon Bedrock\n",
    "\n",
    "Ensure you have the correct IAM permission in order to access OpenAI's models on Amazon Bedrock. In order to ensure you have model access, follow these steps: \n",
    "\n",
    "- Go to Amazon Bedrock --> model access\n",
    "- Click Modify model access\n",
    "- Scroll to OpenAI\n",
    "- Click the checkbox next to the models you would like access to\n",
    "- Click next & accept any EULAs\n",
    "- Click submit\n",
    "\n",
    "## IAM Permissions\n",
    "\n",
    "To use Bedrock models, your AWS credentials need the following permissions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719740df",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:InvokeModel\",\n",
    "        \"bedrock:InvokeModelWithResponseStream\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee373ed",
   "metadata": {},
   "source": [
    "## Step 1: Environment Configuration\n",
    "\n",
    "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
    "\n",
    "### Required Imports:\n",
    "- `os` → For environment variables\n",
    "- `boto3` → For native Bedrock Converse API  \n",
    "- `openai` → For OpenAI SDK compatibility with Bedrock\n",
    "- `strands-agents` → For Amazon Strands agent framework\n",
    "\n",
    "### Environment Variables:\n",
    "We set two environment variables to redirect the OpenAI SDK:\n",
    "- `AWS_BEARER_TOKEN_BEDROCK` → Your Bedrock API key  \n",
    "- `OPENAI_BASE_URL` → Bedrock's OpenAI-compatible endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -U install boto3 openai strands-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906523c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from openai import OpenAI\n",
    "from strands import Agent\n",
    "from strands.models.openai import OpenAIModel\n",
    "from strands.models import BedrockModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882e70b",
   "metadata": {},
   "source": [
    "### Model IDs\n",
    "\n",
    "- **openai.gpt-oss-120b-1:0**\n",
    "- **openai.gpt-oss-20b-1:0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration - Change this to your desired model\n",
    "MODEL_ID = \"gpt-oss-120b\"  \n",
    "\n",
    "print(f\"✅ Using model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables to point to Bedrock\n",
    "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"<insert your bedrock API key>\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
    "\n",
    "print(\"✅ Environment configured for Bedrock!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6caae",
   "metadata": {},
   "source": [
    "## Step 2: Import and Initialize OpenAI Client\n",
    "\n",
    "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
    "\n",
    "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cec1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both clients\n",
    "client = OpenAI()  # For chat completions API\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')  \n",
    "\n",
    "print(\"✅ OpenAI client initialized (pointing to Bedrock)\")\n",
    "print(f\"✅ Bedrock client initialized in region: {bedrock_client.meta.region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12a7c2",
   "metadata": {},
   "source": [
    "## Step 3: Make API Calls \n",
    "\n",
    "The API call structure is identical to OpenAI:\n",
    "- Same `messages` format with `role` and `content`\n",
    "- Same `model` parameter (but uses Bedrock model IDs)  \n",
    "- Same `stream` parameter for real-time responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss-120b\",                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    max_tokens=1000,\n",
    "    stream=True                         \n",
    ")\n",
    "\n",
    "print(\"✅ API request created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a09f4",
   "metadata": {},
   "source": [
    "## Step 4: Process Streaming Response\n",
    "\n",
    "Handle the response exactly like you would with OpenAI. Each `item` in the response is a chunk of the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20620d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in response:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2d0d7",
   "metadata": {},
   "source": [
    "## Bedrock Converse Support\n",
    "\n",
    "\n",
    "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes.\n",
    "\n",
    "Key Benefits:\n",
    "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
    "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
    "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
    "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Say this is a test\"}]\n",
    "        }\n",
    "    ],\n",
    "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0.2,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4547a",
   "metadata": {},
   "source": [
    "## What's Happening Behind the Scenes?\n",
    "\n",
    "When you use the OpenAI SDK with Bedrock, your requests are automatically translated to Bedrock's native `InvokeModel` API.\n",
    "\n",
    "### Request Translation\n",
    "- **OpenAI SDK Request** → **Bedrock InvokeModel** \n",
    "- The request body structure remains the same\n",
    "- But there are some key differences in how parameters are handled:\n",
    "\n",
    "| Parameter | OpenAI SDK | Bedrock InvokeModel |\n",
    "|-----------|------------|-------------------|\n",
    "| **Model ID** | In request body | Part of the URL path |\n",
    "| **Streaming** | `stream=True/False` | Different API endpoints:<br/>• `InvokeModel` (non-streaming)<br/>• `InvokeModelWithResponseStream` (streaming) |\n",
    "| **Request Body** | Full chat completions format | Same format, but `model` and `stream` are optional |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3c82e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You're now using the **OpenAI SDK** with **AWS Bedrock infrastructure**!\n",
    "\n",
    "### What Just Happened?\n",
    "1. **Same Code**: You wrote familiar OpenAI SDK code\n",
    "2. **Different Backend**: Requests went to AWS Bedrock instead of OpenAI\n",
    "3. **Same Experience**: API format, streaming\n",
    "\n",
    "### Benefits\n",
    "- ✅ Use familiar OpenAI SDK patterns\n",
    "- ✅ Run on AWS infrastructure  \n",
    "- ✅ Access to Bedrock's model catalog\n",
    "- ✅ AWS security, logging, and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c253ab2",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "\n",
    "Let's test reasoning with the Chat Completions API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt, temperature=0.02, max_tokens=1000):\n",
    "    \"\"\"Helper function to run prompts with chat completions\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID, # update your model id to either 120b or 20b\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"✅ Helper function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54e021",
   "metadata": {},
   "source": [
    "## Reasoning Use Case Examples\n",
    "\n",
    "Testing complex reasoning and problem-solving capabilities with the Chat Completions API.\n",
    "\n",
    "### Exponential Growth Problem\n",
    "\n",
    "Tests compound growth calculations and iterative reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4766a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Solve and verify.\n",
    "\n",
    "A factory makes widgets. Day 1 output is 500. Each day after, output increases by 8%.\n",
    "How many days until daily output first exceeds 1,000?\n",
    "\n",
    "Return:\n",
    "- Short reasoning outline (≤4 lines)\n",
    "- Final answer (integer days)\n",
    "- One-line self-check (plug back).\"\"\"\n",
    "\n",
    "print(run_prompt(prompt, max_tokens=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff220e",
   "metadata": {},
   "source": [
    "### Kinematics Problem\n",
    "\n",
    "Tests multi-phase motion analysis and distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"A car accelerates uniformly from rest to 27 m/s in 9 s, then coasts at constant speed for 60 s, then brakes uniformly to rest in 6 s.\n",
    "Compute total distance traveled.\n",
    "\n",
    "Return:\n",
    "- Piecewise distances\n",
    "- Sum\n",
    "- One-line unit/sanity check.\"\"\"\n",
    "\n",
    "print(run_prompt(prompt, max_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1800db7",
   "metadata": {},
   "source": [
    "### Scientific Problem Solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_prompt = \"\"\"A ball is thrown vertically upward from ground level with an initial velocity of 20 m/s.\n",
    "Calculate:\n",
    "1. Maximum height reached\n",
    "2. Time to reach maximum height  \n",
    "3. Total time in the air\n",
    "4. Velocity when it returns to ground level\n",
    "\n",
    "Use g = 9.8 m/s². Show your physics reasoning and formulas.\"\"\"\n",
    "\n",
    "print(\"Scientific Problem Solving:\")\n",
    "print(\"=\" * 50)\n",
    "print(run_prompt(science_prompt, max_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a19194",
   "metadata": {},
   "source": [
    "### System Design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b54d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Design Example  \n",
    "system_prompt = \"\"\"Design a URL shortener service like bit.ly for 100M URLs per day.\n",
    "Address:\n",
    "1. Database schema design\n",
    "2. Encoding/decoding algorithm choice\n",
    "3. Scalability considerations (caching, load balancing)\n",
    "4. One potential bottleneck and solution\n",
    "\n",
    "Keep response concise but show your reasoning.\"\"\"\n",
    "\n",
    "print(\"System Design Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(run_prompt(system_prompt, max_tokens=3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c242c2",
   "metadata": {},
   "source": [
    "## Amazon Strands & OpenAI OSS on Amazon Bedrock\n",
    "\n",
    "### Use-case — Two-agent Support-ticket JSON pipeline\n",
    "\n",
    "Strands runs a mini-workflow in which:\n",
    "\n",
    "- Summarizer turns raw Zendesk tickets into a compact JSON array:\n",
    "[{id, summary, severity}]\n",
    "\n",
    "- Triager reads that JSON and returns routing decisions:\n",
    "[{id, route}] where route ∈ {escalate, backlog}\n",
    "\n",
    "Because Strands is model-agnostic, you can swap Claude 3, Titan, or any future Bedrock model by changing one string.\n",
    "\n",
    "Strands itself is an open-source SDK from AWS that gives you single-file agents, built-in multi-agent hand-offs, and first-class Bedrock support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = Agent(\n",
    "    model=bedrock,\n",
    "    system_prompt=(\n",
    "        \"You are SupportSummarizer. \"\n",
    "        \"For each ticket in the list you receive:\\n\"\n",
    "        \"  * Produce JSON with keys id, summary (≤25 words), severity (low|medium|high)\\n\"\n",
    "        \"Return **only** a JSON array.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "triager = Agent(\n",
    "    model=bedrock,\n",
    "    system_prompt=(\n",
    "        \"You are TriageBot. You get a JSON array of tickets that already \"\n",
    "        \"contain severity. For every item output:\\n\"\n",
    "        \"  * id (same as input)\\n\"\n",
    "        \"  * route = 'escalate' if severity is high, else 'backlog'\\n\"\n",
    "        \"Respond with **only** a JSON array.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = [\n",
    "    {\"id\": \"1\", \"text\": \"User cannot log in after resetting password – URGENT, blocks payroll.\"},\n",
    "    {\"id\": \"2\", \"text\": \"Minor typo in footer on marketing site.\"},\n",
    "]\n",
    "\n",
    "# summarise\n",
    "json_summaries = summarizer([t[\"text\"] for t in tickets]).content   \n",
    "\n",
    "# triage\n",
    "routes = triager(json_summaries).content\n",
    "\n",
    "print(\"Summaries:\", json_summaries)\n",
    "print(\"Routes   :\", routes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
