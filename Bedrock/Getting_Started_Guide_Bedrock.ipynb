{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896fda04",
   "metadata": {},
   "source": [
    "# OpenAI OSS Model Getting Started Guide on Amazon Bedrock\n",
    "\n",
    "This notebook provides a comprehensive introduction to using gpt-oss-120b & gpt-oss-20b on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for both model variants. \n",
    "## Model Variants\n",
    "\n",
    "### GPT-OSS-120b\n",
    "\n",
    "Parameters: 120 billion\n",
    "\n",
    "Use Cases: Complex reasoning tasks, agentic use cases\n",
    "\n",
    "### GPT-OSS-20b\n",
    "\n",
    "Parameters: 20 billion\n",
    "\n",
    "Use Cases: Faster inference, cost-effective deployments, simpler tasks\n",
    "\n",
    "## Core Capabilities\n",
    "\n",
    "Both OpenAI model variants share the following characteristics:\n",
    "\n",
    "**Input/Output:** Text-in, text-out \n",
    "\n",
    "**Context Window:** 128,000 tokens  \n",
    "\n",
    "**Model Type:** Advanced reasoning models\n",
    "\n",
    "**Region:** us-west-2\n",
    "\n",
    "**Bedrock Guardrails** ✅ Supported\n",
    "\n",
    "**Converse API** ✅ Supported\n",
    "\n",
    "**OpenAI Chat Completions API** ✅ Supported\n",
    "\n",
    "**Web Search:** ❌ Not available at this time\n",
    "\n",
    "All benchmarking data can be viewed [here](https://openai.com/index/introducing-gpt-oss/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c69a6",
   "metadata": {},
   "source": [
    "## What You'll Learn in this getting started guide\n",
    "\n",
    "- Options to use Amazon Bedrock for GPT-OSS inference, including:    \n",
    "    - Using the OpenAI SDK with Amazon Bedrock\n",
    "    - Using Amazon Bedrock's InvokeModel API\n",
    "    - Using Amazon Bedrock's Converse API\n",
    "- Understanding request parameters and response structures\n",
    "- Choosing between Large and Small models for your use case\n",
    "- Implementing tool calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe38fc",
   "metadata": {},
   "source": [
    "## Model Access on Amazon Bedrock\n",
    "\n",
    "Ensure you have the correct IAM permission in order to access OpenAI's models on Amazon Bedrock. In order to ensure you have model access, follow these steps: \n",
    "\n",
    "- Go to Amazon Bedrock --> model access\n",
    "- Click Modify model access\n",
    "- Scroll to OpenAI\n",
    "- Click the checkbox next to the models you would like access to\n",
    "- Click next & accept any EULAs\n",
    "- Click submit\n",
    "\n",
    "## IAM Permissions\n",
    "\n",
    "To use Bedrock models, your AWS credentials need the following permissions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719740df",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:InvokeModel\",\n",
    "        \"bedrock:InvokeModelWithResponseStream\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee373ed",
   "metadata": {},
   "source": [
    "## Step 1: Environment Configuration\n",
    "\n",
    "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
    "\n",
    "### Required Imports:\n",
    "- `os` → For environment variables\n",
    "- `boto3` → For native Bedrock API interactions  \n",
    "- `json` → For JSON serialization/deserialization\n",
    "- `datetime` → For timestamp tracking and performance measurements\n",
    "- `openai` → For OpenAI SDK compatibility with Bedrock\n",
    "- `IPython.display` → For enhanced output formatting and streaming demonstrations\n",
    "\n",
    "### Environment Variables:\n",
    "We set two environment variables to redirect the OpenAI SDK:\n",
    "- `AWS_BEARER_TOKEN_BEDROCK` → Your Bedrock API key  \n",
    "- `OPENAI_BASE_URL` → Bedrock's OpenAI-compatible endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 openai ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906523c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display, display_markdown, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882e70b",
   "metadata": {},
   "source": [
    "### Model IDs\n",
    "\n",
    "- **openai.gpt-oss-120b-1:0**\n",
    "- **openai.gpt-oss-20b-1:0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration - Change this to your desired model\n",
    "MODEL_ID = \"openai.gpt-oss-120b-1:0\"  \n",
    "\n",
    "print(f\"✅ Using model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb796749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already have AWS_BEARER_TOKEN_BEDROCK in your OS env, this will reuse it.\n",
    "bedrock_key = os.getenv(\"AWS_BEARER_TOKEN_BEDROCK\")\n",
    "if not bedrock_key:\n",
    "    bedrock_key = getpass.getpass(\"Paste your Bedrock API key: \").strip()\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = bedrock_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
    "\n",
    "print(\"✅ Env set for Bedrock OpenAI endpoint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6caae",
   "metadata": {},
   "source": [
    "## Step 2: Inference with Amazon Bedrock\n",
    "\n",
    "### Option 1: OpenAI SDK\n",
    "\n",
    "#### Import and Initialize OpenAI Client\n",
    "\n",
    "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
    "\n",
    "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cec1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()  \n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "print(\"✅ OpenAI client (to Bedrock) ready\")\n",
    "print(\"✅ Bedrock client region:\", bedrock_client.meta.region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12a7c2",
   "metadata": {},
   "source": [
    "#### Make API Calls \n",
    "\n",
    "The API call structure is identical to OpenAI:\n",
    "- Same `messages` format with `role` and `content`\n",
    "- Same `model` parameter (but uses Bedrock model IDs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"openai.gpt-oss-120b-1:0\",                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "    ],\n",
    "    temperature=0.2,                       \n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0c28e",
   "metadata": {},
   "source": [
    "#### What's Happening Behind the Scenes?\n",
    "\n",
    "When you use the OpenAI SDK with Bedrock, your requests are automatically translated to Bedrock's native `InvokeModel` API.\n",
    "\n",
    "#### Request Translation\n",
    "- **OpenAI SDK Request** → **Bedrock InvokeModel** \n",
    "- The request body structure remains the same\n",
    "- But there are some key differences in how parameters are handled:\n",
    "\n",
    "| Parameter | OpenAI SDK | Bedrock InvokeModel |\n",
    "|-----------|------------|-------------------|\n",
    "| **Model ID** | In request body | Part of the URL path |\n",
    "| **Request Body** | Full chat completions format | Same format, but `model` and `stream` are optional |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31281329",
   "metadata": {},
   "source": [
    "### Option 2: Amazon Bedrock's InvokeModel API\n",
    "\n",
    "The Bedrock InvokeModel API is the foundational interface for interacting directly with any model hosted on Amazon Bedrock. It provides low-level, flexible access to model inference, allowing you to send input data and receive generated responses in a consistent way across all supported models.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Direct Access: Interact with any Bedrock model using a unified API endpoint.\n",
    "- Fine-Grained Control: Customize inference parameters and payloads for each request.\n",
    "- Privacy: Amazon Bedrock does not store your input or output data—requests are used only for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a4612",
   "metadata": {},
   "source": [
    "#### Setup client\n",
    "\n",
    "First, we setup the Amazon Bedrock client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76062050",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = None\n",
    "\n",
    "if region is None:\n",
    "    target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "else:\n",
    "    target_region = \"us-west-2\"\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2a1ca",
   "metadata": {},
   "source": [
    "#### Inference with InvokeModel API\n",
    "\n",
    "Then we use the InvokeModel API to perform model inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5121a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    \"\"\"\n",
    "    Invokes Amazon bedrock model to run an inference\n",
    "    using the input provided in the request body.\n",
    "    \n",
    "    Args:\n",
    "        body (dict): The invokation body to send to bedrock\n",
    "        model_id (str): the model to query\n",
    "        accept (str): input accept type\n",
    "        content_type (str): content type\n",
    "    Returns:\n",
    "        Inference response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the largest city in the northern hemisphere?\"}\n",
    "]\n",
    "  \n",
    "# OpenAI OSS models on Bedrock expect OpenAI Chat Completions format\n",
    "body = {\n",
    "    \"messages\": messages,        \n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "modelId = \"openai.gpt-oss-120b-1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, modelId, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "# OpenAI format response\n",
    "print(response_body[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2d0d7",
   "metadata": {},
   "source": [
    "### Option 3: Amazon Bedrock's Converse API\n",
    "\n",
    "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes.\n",
    "\n",
    "Key Benefits:\n",
    "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
    "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
    "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
    "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Where is our galaxy relative to others in the Milky Way?\"}]\n",
    "        }\n",
    "    ],\n",
    "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    ")\n",
    "\n",
    "content = response.get(\"output\", {}).get(\"message\", {}).get(\"content\", [])\n",
    "print(\"\".join(block.get(\"text\", \"\") for block in content).strip() or \"(no text)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3c82e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully explored **three powerful ways** to interact with OpenAI's GPT-OSS models on Amazon Bedrock, including comprehensive tool use capabilities!\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "**1. OpenAI SDK Integration**\n",
    "- Set up the familiar OpenAI SDK to work seamlessly with AWS Bedrock\n",
    "- Leveraged existing OpenAI patterns while running on AWS infrastructure\n",
    "- Demonstrated streaming responses with real-time token generation\n",
    "- **Implemented function calling** using familiar OpenAI SDK patterns\n",
    "\n",
    "**2. Direct InvokeModel API Access**\n",
    "- Implemented low-level Bedrock InvokeModel API calls for maximum control\n",
    "- Built custom functions for both streaming and non-streaming inference\n",
    "- Measured performance metrics like time-to-first-token for streaming responses\n",
    "- Gained fine-grained control over model parameters and request formatting\n",
    "- **Built custom tool use handling** with manual request/response processing\n",
    "\n",
    "**3. Bedrock Converse API**\n",
    "- Explored the unified Converse API that works across all Bedrock models\n",
    "- Demonstrated consistent message-based interactions regardless of underlying model\n",
    "- Leveraged built-in support for system prompts and inference configuration\n",
    "- **Integrated tool calling** using Bedrock's native toolSpec format\n",
    "\n",
    "### Key Benefits Achieved\n",
    "\n",
    "✅ **Flexibility**: Three different API approaches for different use cases  \n",
    "✅ **Performance**: Streaming support for improved user experience  \n",
    "✅ **Familiarity**: Use existing OpenAI SDK patterns with AWS infrastructure  \n",
    "✅ **Control**: Direct API access when you need fine-grained customization  \n",
    "✅ **Consistency**: Universal interface that works across all Bedrock models  \n",
    "✅ **Privacy**: AWS Bedrock doesn't store your data - only used for inference  \n",
    "✅ **Practical Comparison**: Side-by-side examples using the same function\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You're now equipped with comprehensive knowledge to choose the right API approach for your specific use case. Whether you need:\n",
    "- The **simplicity** of the OpenAI SDK\n",
    "- The **control** of InvokeModel \n",
    "- The **consistency** of Converse API\n",
    "\n",
    "You have all the tools and examples to build powerful AI applications with external function calling on AWS Bedrock!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c253ab2",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "\n",
    "Let's test reasoning with the Chat Completions API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt, temperature=0.02):\n",
    "    \"\"\"Helper function to run prompts with chat completions\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID, # update your model id to either 120b or 20b\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"✅ Helper function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54e021",
   "metadata": {},
   "source": [
    "## Reasoning Use Case Examples\n",
    "\n",
    "Testing complex reasoning and problem-solving capabilities with the Chat Completions API.\n",
    "\n",
    "### Exponential Growth Problem\n",
    "\n",
    "Tests compound growth calculations and iterative reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4766a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Solve and verify.\n",
    "\n",
    "A factory makes widgets. Day 1 output is 500. Each day after, output increases by 8%.\n",
    "How many days until daily output first exceeds 1,000?\n",
    "\n",
    "Return:\n",
    "- Short reasoning outline (≤4 lines)\n",
    "- Final answer (integer days)\n",
    "- One-line self-check (plug back).\"\"\"\n",
    "\n",
    "print(run_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff220e",
   "metadata": {},
   "source": [
    "### Kinematics Problem\n",
    "\n",
    "Tests multi-phase motion analysis and distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"A car accelerates uniformly from rest to 27 m/s in 9 s, then coasts at constant speed for 60 s, then brakes uniformly to rest in 6 s.\n",
    "Compute total distance traveled.\n",
    "\n",
    "Return:\n",
    "- Piecewise distances\n",
    "- Sum\n",
    "- One-line unit/sanity check.\"\"\"\n",
    "\n",
    "print(run_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1800db7",
   "metadata": {},
   "source": [
    "### Scientific Problem Solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_prompt = \"\"\"A ball is thrown vertically upward from ground level with an initial velocity of 20 m/s.\n",
    "Calculate:\n",
    "1. Maximum height reached\n",
    "2. Time to reach maximum height  \n",
    "3. Total time in the air\n",
    "4. Velocity when it returns to ground level\n",
    "\n",
    "Use g = 9.8 m/s². Show your physics reasoning and formulas.\"\"\"\n",
    "\n",
    "print(\"Scientific Problem Solving:\")\n",
    "print(\"=\" * 50)\n",
    "print(run_prompt(science_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a19194",
   "metadata": {},
   "source": [
    "### System Design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b54d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Design Example  \n",
    "system_prompt = \"\"\"Design a URL shortener service like bit.ly for 100M URLs per day.\n",
    "Address:\n",
    "1. Database schema design\n",
    "2. Encoding/decoding algorithm choice\n",
    "3. Scalability considerations (caching, load balancing)\n",
    "4. One potential bottleneck and solution\n",
    "\n",
    "Keep response concise but show your reasoning.\"\"\"\n",
    "\n",
    "print(\"System Design Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(run_prompt(system_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
