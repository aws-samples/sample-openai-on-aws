{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be733679-b588-411c-b30c-558a5d19ce06",
   "metadata": {},
   "source": [
    "## Deploying GPT-OSS Models with SMJS and SageMaker Inference Components\n",
    "\n",
    "This notebook demonstrates how to deploy OpenAI’s GPT‑OSS models (20B) on Amazon SageMaker JumpStart using **inference components**. Inference components let you host multiple copies of a model—or even different models—on a single endpoint and route traffic to specific components.\n",
    "\n",
    "**SM Inference Components**\n",
    "\n",
    "![IC](imgs/sm-mme-01.png)\n",
    "\n",
    "These models provide a 128 k token context window and support configurable reasoning levels (low, medium, or high). To enable web search grounding, disable network isolation and supply an EXA API key via the `EXA_API_KEY` environment variable when deploying the model.\n",
    "\n",
    "**gpt-oss-20b in SMJS console**\n",
    "\n",
    "![IC](imgs/smjs-img.png)\n",
    "\n",
    "In the deployment example below we provision a single `p5.48xlarge` instance (since this is the SMJS default) and attach two 20B inference components using one GPU each (tensor parallel degree = 1). You can route inference to a particular component by specifying its `InferenceComponentName` when you invoke the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad50841-d8cc-4c17-9d45-733f0c04cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "smrt = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"oai-oss-mix\")\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404205c",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "\n",
    "You can either execute this notebook in SageMaker Studio or locally. If you are new to Amazon SageMaker, please follow the guidance provided here: [Guide to getting set up with Amazon SageMaker\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html). \n",
    "\n",
    "For local setup please make sure to configure configure the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your credentials, install necessary libraries and ensure you have [permissions for SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n",
    "In addition to either setting up your local environment with the local permissions or setting up SageMaker Studio please also make sure to check instance quota. This Jupyter Notebook itself can be run on a ml.t3.medium instance. To deploy the model to the SageMaker endpoint, you may need to request a quota increase. To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the Service Quotas console.\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "    - ml.p5.48xlarge for endpoint usage\n",
    "4. If needed, request a quota increase for these resources.\n",
    "5. By default, models in SageMaker JumpStart run in network isolation. The GPT OSS models come with a built-in tool for web search using EXA, a meaning-based web search API powered by embeddings. To use this tool, OpenAI requires customers get an [API key from EXA](https://exa.ai/?gad_source=1&gad_campaignid=22502546132&gbraid=0AAAAA-z0DiAZu9OD-gshUnVtITyrtBazd&gclid=CjwKCAjw49vEBhAVEiwADnMbbJJ0-UITnd31rsQ1gyGsCQA5LWWcQuY5y6hbtFpjAMOz82wlqdGCcxoCCQsQAvD_BwE) and pass this key as an environment variable to their JumpStartModel instance when deploying it through the SageMaker Python SDK. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293a31c",
   "metadata": {},
   "source": [
    "Let's spin up a an endpoint with two model copies at `tp=1`. For context, a `p5.48xlarge` has 8 H100s so you're able to go up to 8 model copies per instance for `gpt-oss-20b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18d4bbd9-9fef-4e87-8ae7-7e458a314ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'openai-reasoning-gpt-oss-20b' with wildcard version identifier '*'. You can pin to version '1.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'openai-reasoning-gpt-oss-20b' with wildcard version identifier '*'. You can pin to version '1.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p5.48xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.p5.48xlarge.\n",
      "INFO:sagemaker:Creating model with name: oai-oss-mix-2025-08-08-15-55-27-742-20b\n",
      "INFO:sagemaker:Creating inference component with name ic-20b-oai-oss-mix-2025-08-08-15-55-27-742 for endpoint oai-oss-mix-2025-08-08-15-55-27-742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------!"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Inference Component: 20B (2 copies @ TP=1)\n",
    "# ---------------------------\n",
    "ic_20b_name  = f\"ic-20b-{endpoint_name}\"\n",
    "model_20b_id = \"openai-reasoning-gpt-oss-20b\"      # JumpStart id (OAI GPT-OSS 20B)\n",
    "\n",
    "env_20b = {\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\",\n",
    "    \"EXA_API_KEY\": \"<insert your Exa api key>\"\n",
    "}\n",
    "\n",
    "js_20b = JumpStartModel(\n",
    "    model_id=model_20b_id,\n",
    "    model_version=model_version,\n",
    "    enable_network_isolation=False,\n",
    "    name=f\"{endpoint_name}-20b\",\n",
    "    env=env_20b,\n",
    ")\n",
    "\n",
    "_ = js_20b.deploy(\n",
    "    accept_eula=True,\n",
    "    instance_type=\"ml.p5.48xlarge\",                 # same endpoint/instance\n",
    "    initial_instance_count=1,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    endpoint_name=endpoint_name,                    # attach to existing endpoint\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=ic_20b_name,\n",
    "    resources=ResourceRequirements(\n",
    "        requests={\n",
    "            \"num_accelerators\": 1,  # per-copy GPU reservation\n",
    "            \"memory\": 8192,\n",
    "            \"copies\": 2             # two 20B copies; SageMaker LB across them\n",
    "        }\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b99fcdd-447c-4a76-bd4b-80ed2ce534a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: oai-oss-mix-2025-08-08-15-55-27-742\n",
      "IC 20B: ic-20b-oai-oss-mix-2025-08-08-15-55-27-742\n",
      "IC 20B : ic-20b-oai-oss-mix-2025-08-08-15-55-27-742\n"
     ]
    }
   ],
   "source": [
    "print(\"Endpoint:\", endpoint_name)\n",
    "print(\"IC 20B:\", ic_20b_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de6b41-ec32-4f73-947f-ea663a979b67",
   "metadata": {},
   "source": [
    "## Harmony response format and inference examples\n",
    "\n",
    "When interacting with GPT‑OSS models you must use the **Harmony response format**. Each message has a **role** and may specify a **channel**. Harmony supports five roles:\n",
    "\n",
    "| Role | Purpose |\n",
    "|-----|---------|\n",
    "| **system** | Specifies the model’s identity, knowledge cutoff, current date, reasoning level, available channels and built‑in tools. |\n",
    "| **developer** | Provides instructions for the model (similar to a system prompt) and declares available function tools. |\n",
    "| **user** | Represents the end‑user’s input. |\n",
    "| **assistant** | Represents the model’s output; may be a tool call or a message. |\n",
    "| **tool** | Contains the output returned by an external tool; the tool name itself is used as the role. |\n",
    "\n",
    "Assistant messages can be tagged with one of three channels:\n",
    "\n",
    "| Channel | Purpose |\n",
    "|-------|---------|\n",
    "| **analysis** | Holds the model’s chain‑of‑thought reasoning. These messages are **not** intended for end‑users. |\n",
    "| **commentary** | Used for function/tool calls and sometimes for preambles before calling multiple tools. |\n",
    "| **final** | Contains the user‑visible answer or output. |\n",
    "\n",
    "### Example inference payloads\n",
    "\n",
    "The example code in this notebook demonstrates a simple function call flow:\n",
    "\n",
    "1. Compose a single string that concatenates the system message, the developer message (with tool definitions), the user’s question and an `Assistant:` marker.\n",
    "2. Define the same function in the `tools` array of the payload.\n",
    "3. Invoke the model. It responds with a `function_call` specifying the tool name and arguments on the commentary channel.\n",
    "4. Execute the tool on the client, format its JSON result, and send it back in a tool message (role =`tool`).\n",
    "5. Invoke the model again with the appended tool message. It replies on the final channel with a human‑readable answer.\n",
    "\n",
    "After an assistant message in the final channel you should drop any previous chain‑of‑thought content when forming the next request. If the last message was a tool call, include both the chain‑of‑thought and the tool call before you continue the conversation.\n",
    "\n",
    "In the following code cells we route inference to the `ic‑20b` inference component and demonstrate the EXA‑powered `web_search_preview` tool (enabled via the `EXA_API_KEY` in the environment) to answer questions with citations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ab9fc",
   "metadata": {},
   "source": [
    "The **System Message** is to specify built-in tools, available channels, etc. It is different from the system prompt. The built-in tools are specified in typescript format. Here we use the built-in web search tool with **EXA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6a157ee-9e50-4804-95a9-6fa194c40513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "CURRENT_DATE = date.today().isoformat()\n",
    "\n",
    "SYSTEM_CORE = (\n",
    "    \"You are ChatGPT, a large language model trained by OpenAI.\\n\"\n",
    "    \"Knowledge cutoff: 2024-06\\n\"\n",
    "    f\"Current date: {CURRENT_DATE}\\n\\n\"\n",
    "    \"reasoning: medium\\n\\n\"\n",
    "    \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\\n\"\n",
    "    \"Calls to these tools must go to the commentary channel: 'functions'.\"\n",
    ")\n",
    "\n",
    "# Built-in browser tool \n",
    "BROWSER_BLOCK = \"\"\"\n",
    "# Tools\n",
    "## browser\n",
    "// Tool for browsing.\n",
    "// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\n",
    "// Cite information from the tool using the following format:\n",
    "// `【{cursor}†L{line_start}(-L{line_end})?】`.\n",
    "// Do not quote more than 10 words directly from the tool output.\n",
    "// sources=web (default: web)\n",
    "namespace browser {\n",
    "// Searches for information related to `query` and displays `topn` results.\n",
    "type search = (_: {\n",
    "query: string,\n",
    "topn?: number, // default: 10\n",
    "source?: string,\n",
    "}) => any;\n",
    "// Opens the link `id` from the page indicated by `cursor`.\n",
    "type open = (_: {\n",
    "id?: number | string, // default: -1\n",
    "cursor?: number, // default: -1\n",
    "loc?: number, // default: -1\n",
    "num_lines?: number, // default: -1\n",
    "view_source?: boolean, // default: false\n",
    "source?: string,\n",
    "}) => any;\n",
    "// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\n",
    "type find = (_: {\n",
    "pattern: string,\n",
    "cursor?: number, // default: -1\n",
    "}) => any;\n",
    "} // namespace browser\n",
    "\"\"\"\n",
    "def build_system_message(include_browser=True, include_python=False) -> str:\n",
    "    parts = [SYSTEM_CORE]\n",
    "    if include_browser:\n",
    "        parts.append(BROWSER_BLOCK)\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbda2ea",
   "metadata": {},
   "source": [
    "The **Developer Message** is where you specify the function calls and system prompt. Functions are typically in typescript.\n",
    "\n",
    "All functions that are available to the model should be defined in the developer message in a dedicated Tools section.\n",
    "To define the functions we use a TypeScript-like type syntax and wrap the functions into a dedicated functions namespace. It’s important to stick to this format closely to improve accuracy of function calling. You can check out the harmony renderer codebase for more information on how we are turning JSON schema definitions for the arguments into this format but some general formatting practices:\n",
    "\n",
    "* Define every function as a type {function_name} = () => any if it does not receive any arguments\n",
    "* For functions that receive an argument name the argument _ and inline the type definition\n",
    "* Add comments for descriptions in the line above the field definition\n",
    "* Always use any as the return type\n",
    "* Keep an empty line after each function definition\n",
    "* Wrap your functions into a namespace, generally functions is the namespace you should use to not conflict with other tools that the model might have been trained on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2f69cf8-8727-4a22-84b1-04d14a574291",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_INSTRUCTIONS = \"You are a helpful AI assistant. Provide clear, concise, and helpful responses.\"\n",
    "\n",
    "FUNCTIONS_BLOCK = \"\"\"\n",
    "# Tools\n",
    "## functions\n",
    "namespace functions {\n",
    "// Gets the current weather for a specific location.\n",
    "type get_current_weather = (_: {\n",
    "// The city and state/country, e.g. \"San Francisco, CA\" or \"London, UK\"\n",
    "location: string,\n",
    "// Temperature unit preference\n",
    "unit?: \"celsius\" | \"fahrenheit\", // default: celsius\n",
    "}) => any;\n",
    "} // namespace functions\n",
    "\"\"\"\n",
    "\n",
    "def build_developer_message(include_functions=True) -> str:\n",
    "    body = [\"# Instructions\", DEVELOPER_INSTRUCTIONS]\n",
    "    if include_functions:\n",
    "        body.append(FUNCTIONS_BLOCK)\n",
    "    return \"\\n\\n\".join(body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1948c9f5-e6e3-4cfb-ba04-3b30aac027e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(system_msg: str, developer_msg: str, user_text: str) -> str:\n",
    "    return (\n",
    "        f\"System: {system_msg.strip()}\\n\\n\"\n",
    "        f\"Developer: {developer_msg.strip()}\\n\\n\"\n",
    "        f\"Human: {user_text.strip()}\\n\\n\"\n",
    "        \"Assistant:\"\n",
    "    )\n",
    "\n",
    "SYSTEM_MSG = build_system_message(include_browser=True)\n",
    "DEVELOPER_MSG = build_developer_message(include_functions=True)\n",
    "\n",
    "INPUT_STR_WEATHER_TOOL = build_input(\n",
    "    SYSTEM_MSG,\n",
    "    DEVELOPER_MSG,\n",
    "    \"What is the weather like in Seattle?\"\n",
    ")\n",
    "\n",
    "INPUT_STR_BROWSER_TOOL = build_input(\n",
    "    SYSTEM_MSG,\n",
    "    DEVELOPER_MSG,\n",
    "    \"Who is the current President of the US? Use Browser tool.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199744e7",
   "metadata": {},
   "source": [
    "#### Weather function call example\n",
    "\n",
    "The following cell sends a Harmony-formatted request to the 20B inference component using the `get_current_weather` tool.\n",
    "\n",
    "We build a single-string prompt that includes the system and developer messages, declare the tool in the `tools` array, and ask about the weather. The model will respond with a tool call in the commentary channel, specifying the location and unit. We print the raw JSON response to inspect the Harmony structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b65c2e79-ac48-45b6-a4ad-d638b0f5fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "TOOLS_ARRAY = [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Gets the current weather for a specific location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state/country, e.g. 'San Francisco, CA' or 'London, UK'\"\n",
    "          },\n",
    "          \"unit\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "            \"default\": \"celsius\",\n",
    "            \"description\": \"Temperature unit preference\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    }\n",
    "]\n",
    "\n",
    "payload_weather = {\n",
    "  \"model\": \"/opt/ml/model\",\n",
    "  \"input\": INPUT_STR_WEATHER_TOOL,                                 \n",
    "  \"instructions\": DEVELOPER_INSTRUCTIONS,             \n",
    "  \"max_output_tokens\": 2048,\n",
    "  \"stream\": \"false\",\n",
    "  \"temperature\": 0.7,\n",
    "  \"reasoning\": {\"effort\": \"medium\"},\n",
    "  \"tool_choice\": \"auto\",\n",
    "  \"tools\": TOOLS_ARRAY\n",
    "}\n",
    "\n",
    "payload_browser = {\n",
    "  \"model\": \"/opt/ml/model\",\n",
    "  \"input\": INPUT_STR_BROWSER_TOOL,                                \n",
    "  \"instructions\": DEVELOPER_INSTRUCTIONS,             \n",
    "  \"max_output_tokens\": 2048,\n",
    "  \"stream\": \"false\",\n",
    "  \"temperature\": 0.7,\n",
    "  \"reasoning\": {\"effort\": \"medium\"},\n",
    "  \"tool_choice\": \"auto\",\n",
    "  \"tools\": TOOLS_ARRAY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6836d41d-7ba9-4960-adb9-5d4c0201739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"resp_70830f03b78847d8aa20d4520b0a759d\",\"created_at\":1754779573,\"instructions\":\"You are a helpful AI assistant. Provide clear, concise, and helpful responses.\",\"metadata\":null,\"model\":\"/opt/ml/model\",\"object\":\"response\",\"output\":[{\"id\":\"rs_3ef4c581c73e4672a7e7b6695b00401c\",\"content\":[{\"text\":\"The user asks: \\\"What is the weather like in Seattle?\\\" We have a function to get current weather. So we need to call it.\",\"type\":\"reasoning_text\"}],\"summary\":[],\"type\":\"reasoning\",\"encrypted_content\":null,\"status\":null},{\"arguments\":\"{\\\"location\\\":\\\"Seattle, WA\\\",\\\"unit\\\":\\\"celsius\\\"}\",\"call_id\":\"call_b363c9d6206c4fec9bfa0789b1186ff6\",\"name\":\"get_current_weather\",\"type\":\"function_call\",\"id\":\"ft_b363c9d6206c4fec9bfa0789b1186ff6\",\"status\":null}],\"parallel_tool_calls\":true,\"temperature\":0.7,\"tool_choice\":\"auto\",\"tools\":[{\"name\":\"get_current_weather\",\"parameters\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state/country, e.g. 'San Francisco, CA' or 'London, UK'\"},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"],\"default\":\"celsius\",\"description\":\"Temperature unit preference\"}},\"required\":[\"location\"]},\"strict\":null,\"type\":\"function\",\"description\":\"Gets the current weather for a specific location\"}],\"top_p\":1.0,\"background\":false,\"max_output_tokens\":2048,\"max_tool_calls\":null,\"previous_response_id\":null,\"prompt\":null,\"reasoning\":{\"effort\":\"medium\",\"generate_summary\":null,\"summary\":null},\"service_tier\":\"auto\",\"status\":\"completed\",\"text\":null,\"top_logprobs\":0,\"truncation\":\"disabled\",\"usage\":{\"prompt_tokens\":0,\"total_tokens\":0,\"completion_tokens\":0,\"prompt_tokens_details\":null},\"user\":null}\n"
     ]
    }
   ],
   "source": [
    "response = smrt.invoke_endpoint(\n",
    "    EndpointName=\"oai-oss-mix-2025-08-08-15-55-27-742\",\n",
    "    InferenceComponentName=\"ic-20b-oai-oss-mix-2025-08-08-15-55-27-742\",  # <- set to route to a specific IC if you want\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(payload_weather),\n",
    ")\n",
    "\n",
    "print(response[\"Body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4803213",
   "metadata": {},
   "source": [
    "#### Browser use example\n",
    "\n",
    "The following cell sends a Harmony-formatted request to the 20B inference component and prompts the use of the built-in web search tool.\n",
    "\n",
    "If the model decides to call actions in the browser it will use the same format as for function calls with two notable exceptions:\n",
    "\n",
    "1. Requests will be made to the analysis channel\n",
    "2. The recipient will be `browser.search`, `browser.open`, `browser.find` respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2369836b-8240-4632-8c08-07d7e1469288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"resp_7ee668a090154baa8863b3421d6e4275\",\"created_at\":1754779604,\"instructions\":\"You are a helpful AI assistant. Provide clear, concise, and helpful responses.\",\"metadata\":null,\"model\":\"/opt/ml/model\",\"object\":\"response\",\"output\":[{\"id\":\"rs_a99d3df16e934a2bab0f835e0acfb2c4\",\"content\":[{\"text\":\"The user asks: \\\"Who is the current President of the US? Use Browser tool.\\\" They want us to browse and provide a current answer. The current date is 2025-08-09. So we need to verify that the current President is Joe Biden? Wait, Joe Biden was elected in 2020, inaugurated 2021. But the next election is 2024, and likely the next president might be someone else. As of 2025, it's likely Joe Biden still is president? Let's check. The term for a US president is 4 years. Biden started 20 Jan 2021. His term ended 20 Jan 2025. If he didn't run, someone else could be president. In 2024, the winner was Donald Trump or someone else. Actually as of 2025, the current president might be someone else. Let's search.\",\"type\":\"reasoning_text\"}],\"summary\":[],\"type\":\"reasoning\",\"encrypted_content\":null,\"status\":null},{\"id\":\"ws_9aa72d49a92e4823a65c860880f84f2e\",\"action\":{\"query\":\"current President of the United States 2025\",\"type\":\"search\"},\"status\":\"completed\",\"type\":\"web_search_call\"},{\"id\":\"rs_fc42ffcf573a49149254626a86858dd0\",\"content\":[{\"text\":\"We should open a source to cite. Let's open Wikipedia link id 0.\",\"type\":\"reasoning_text\"}],\"summary\":[],\"type\":\"reasoning\",\"encrypted_content\":null,\"status\":null},{\"id\":\"rs_48d6ae20f4454abc9dbc41e2cb27fdba\",\"content\":[{\"text\":\"Scrolling near lines 60-64 shows that Donald Trump is incumbent as of January 20, 2025. Good. We can cite that. Also maybe open the White House link. But Wikipedia is enough. Let's confirm another source. Open USA.gov link id 1.\",\"type\":\"reasoning_text\"}],\"summary\":[],\"type\":\"reasoning\",\"encrypted_content\":null,\"status\":null},{\"id\":\"rs_a82ed94326414549b1dacd44780ae666\",\"content\":[{\"text\":\"Lines 43-45 show Donald Trump is current president. Let's cite that. Good.\\n\\nNow answer: The current President is Donald John Trump, the 47th President, sworn in on Jan 20, 2025.\\n\\nWe should provide that with citations.\",\"type\":\"reasoning_text\"}],\"summary\":[],\"type\":\"reasoning\",\"encrypted_content\":null,\"status\":null},{\"id\":\"msg_4331edcd55f84c38b7ce4c17d0ef7a34\",\"content\":[{\"annotations\":[],\"text\":\"The current President of the United States is **Donald John Trump** (the 47th president). He was sworn into office on January 20, 2025【19†L61-L64】【20†L43-L45】.\",\"type\":\"output_text\",\"logprobs\":null}],\"role\":\"assistant\",\"status\":\"completed\",\"type\":\"message\"}],\"parallel_tool_calls\":true,\"temperature\":0.7,\"tool_choice\":\"auto\",\"tools\":[{\"name\":\"get_current_weather\",\"parameters\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state/country, e.g. 'San Francisco, CA' or 'London, UK'\"},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"],\"default\":\"celsius\",\"description\":\"Temperature unit preference\"}},\"required\":[\"location\"]},\"strict\":null,\"type\":\"function\",\"description\":\"Gets the current weather for a specific location\"}],\"top_p\":1.0,\"background\":false,\"max_output_tokens\":126860,\"max_tool_calls\":null,\"previous_response_id\":null,\"prompt\":null,\"reasoning\":{\"effort\":\"medium\",\"generate_summary\":null,\"summary\":null},\"service_tier\":\"auto\",\"status\":\"completed\",\"text\":null,\"top_logprobs\":0,\"truncation\":\"disabled\",\"usage\":{\"prompt_tokens\":0,\"total_tokens\":0,\"completion_tokens\":0,\"prompt_tokens_details\":null},\"user\":null}\n"
     ]
    }
   ],
   "source": [
    "response = smrt.invoke_endpoint(\n",
    "    EndpointName=\"oai-oss-mix-2025-08-08-15-55-27-742\",\n",
    "    InferenceComponentName=\"ic-20b-oai-oss-mix-2025-08-08-15-55-27-742\",  # <- set to route to a specific IC if you want\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(payload_browser),\n",
    ")\n",
    "\n",
    "print(response[\"Body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b21a4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a0ca4-5611-4cef-b3e4-c89d90fd78ca",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The outputs above illustrate how GPT‑OSS models use the Harmony format to separate reasoning, tool usage, and final answers.\n",
    "\n",
    "In the first invocation, the assistant produces reasoning in the analysis channel, then emits a tool call on the commentary channel. In the second invocation, a built-in web search is triggered via the EXA‑powered tool, and citations are included in the final response.\n",
    "\n",
    "This concludes our demonstration of deploying GPT‑OSS models on SageMaker, invoking them with Harmony‑compliant prompts, and handling function calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
