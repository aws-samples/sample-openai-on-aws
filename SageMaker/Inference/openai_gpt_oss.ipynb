{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy OpenAI gpt-oss model on SageMaker AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef819c-5aff-4e14-a07d-27c227204560",
   "metadata": {},
   "source": [
    "In this notebook we deploy OpenAI gpt-oss model on SageMaker AI using several options.\n",
    "\n",
    "Please see the OpenAI introduction [blog](https://simonwillison.net/2025/Aug/5/gpt-oss/) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a1755-ddcb-416c-aa74-33920280fed2",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Fetch and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4039c65-7c85-496b-9f4e-9b7d29bacdb5",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c44ad6-de5f-42c9-acef-67d525741307",
   "metadata": {},
   "source": [
    "### Option 1. Deploy gpt-oss-120b from JumpStart \n",
    "\n",
    "We will use Inference Component enabled endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7246627-0df7-4326-91ce-4a9db828a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "accept_eula = False  # Change to True to agree to term and conditions and accept EULA\n",
    "model_id, model_version = \"openai-reasoning-gpt-oss-120b\", \"1.0.0\"\n",
    "\n",
    "model_name = endpoint_name = sagemaker.utils.name_from_base(\"gpt-oss-120b\")\n",
    "inference_component_name = f\"ic-{model_name}\"\n",
    "\n",
    "jumpstart_model = JumpStartModel(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "jumpstart_model.deploy(\n",
    "    accept_eula=accept_eula,\n",
    "    instance_type=\"ml.p5en.48xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=inference_component_name,\n",
    "    resources=ResourceRequirements(requests={\"num_accelerators\": 8, \"memory\": 1024*10, \"copies\": 1,}),\n",
    ")\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    component_name=inference_component_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d14498-2e3c-455f-b291-e2b66be72f5c",
   "metadata": {},
   "source": [
    "### Option 2. Deploy gpt-oss model from S3\n",
    "\n",
    "If you need to deploy these models from S3 (for example, after fine-tuning) you can use the code below.\n",
    "\n",
    "Please change the `model_s3_path` to the S3 prefix with your model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa137e-dc85-4b4b-b457-47a06b219452",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.0.0.dev1-lmi0.0.0-cu128\"\n",
    "instance_type = \"ml.p5en.48xlarge\"\n",
    "\n",
    "model_s3_path = \"s3://<BUCKET>/<PREFIX>/\"\n",
    "\n",
    "lmi_env = {\n",
    "    \"OPTION_MODEL_ID\": \"/opt/ml/model\",\n",
    "    #\"TIKTOKEN_ENCODINGS_BASE\": \"/opt/ml/model\",\n",
    "    \"OPTION_TENSOR_PARALLEL_SIZE\": \"8\",\n",
    "}\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(\"model-lmi\")\n",
    "endpoint_name = model_name\n",
    "inference_component_name = f\"ic-{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7a8cf-ce28-46e1-a716-a0d30abff790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "lmi_model = sagemaker.Model(\n",
    "    image_uri=inference_image,\n",
    "    env=lmi_env,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    model_data={\n",
    "        'S3DataSource': {\n",
    "            'S3Uri': model_s3_path,\n",
    "            'S3DataType': 'S3Prefix',\n",
    "            'CompressionType': 'None'\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=600,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=inference_component_name,\n",
    "    resources=ResourceRequirements(requests={\"num_accelerators\": 8, \"memory\": 1024*10, \"copies\": 1,}),\n",
    ")\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    component_name=inference_component_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b070f66-bbe8-4f6b-8143-190a36bf31e0",
   "metadata": {},
   "source": [
    "### Option 3. Deploying from HF using BYOC\n",
    "\n",
    "Amazon SageMaker AI provides the ability to build Docker containers to run on SageMaker endpoints, where they listen for health checks on `/ping` and receive real-time inference requests on `/invocations`.\n",
    "\n",
    "Below, we'll demonstrate how to adapt the [vLLM](https://github.com/vllm-project/vllm) framework to run on SageMaker AI endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93294b-e807-445e-8b79-85c7a91d4b5b",
   "metadata": {},
   "source": [
    "#### Container preparation:\n",
    "1. Enable Docker access in your Studio domain. Please see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-local-get-started.html#studio-updated-local-enable) for details\n",
    "2. Install Docker in your Studion environment. See this [link](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-local-get-started.html#studio-updated-local-docker-installation) for details\n",
    "3. Please modify `build.sh` to change the account_id, region, repository name, and tag if required\n",
    "4. Build the image and push to ECR repository using `build.sh` in docker directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b18a7-c0be-4ab0-8fa1-be907ab959f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/build.sh\n",
    "export REGION=YOUR_REGION\n",
    "export ACCOUNT_ID=YOUR_ACCOUNT_ID\n",
    "export REPOSITORY_NAME=vllm\n",
    "export TAG=v0.10.0-gpt-oss\n",
    "\n",
    "full_name=\"${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPOSITORY_NAME}:${TAG}\"\n",
    "\n",
    "DOCKER_BUILDKIT=1 docker build . --tag $REPOSITORY_NAME:$TAG --file Dockerfile.gptoss\n",
    "\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --region ${REGION} --repository-names \"${REPOSITIRY_NAME}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --region ${REGION} --repository-name \"${REPOSITORY_NAME}\" > /dev/null\n",
    "fi\n",
    "\n",
    "docker tag $REPOSITORY_NAME:$TAG ${full_name}\n",
    "docker push ${full_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af4d9f-8757-4d84-99f8-e8348ad015d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd docker; ./build.sh; cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee101e-52fb-4f92-834d-11cef5b53700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Please make sure you are using the image that you pushed into ECR in a previous step\n",
    "#\n",
    "inference_image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:v0.10.0-gpt-oss\"\n",
    "instance_type = \"ml.g6e.4xlarge\"\n",
    "num_gpu = 1\n",
    "model_name = sagemaker.utils.name_from_base(\"model-byoc\")\n",
    "endpoint_name = model_name\n",
    "inference_component_name = f\"ic-{model_name}\"\n",
    "\n",
    "config = {\n",
    "    \"OPTION_MODEL\": \"openai/gpt-oss-20b\",\n",
    "    \"OPTION_SERVED_MODEL_NAME\": \"model\",\n",
    "    \"OPTION_TENSOR_PARALLEL_SIZE\": json.dumps(num_gpu),\n",
    "    \"VLLM_ATTENTION_BACKEND\": \"TRITON_ATTN_VLLM_V1\",\n",
    "    \"OPTION_ASYNC_SCHEDULING\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ae72d-4e66-442d-af11-0d0e4020f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "lmi_model = sagemaker.Model(\n",
    "    image_uri=inference_image,\n",
    "    env=config,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=600,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=inference_component_name,\n",
    "    resources=ResourceRequirements(requests={\"num_accelerators\": num_gpu, \"memory\": 1024*5, \"copies\": 1,}),\n",
    ")\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    component_name=inference_component_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fde0a-1a56-4f2b-95b8-bc98d7854d5c",
   "metadata": {},
   "source": [
    "## Inference Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "533da82f-f84b-4d2a-8a23-ecc344929f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T19:49:16.939394Z",
     "iopub.status.busy": "2025-08-10T19:49:16.939063Z",
     "iopub.status.idle": "2025-08-10T19:49:21.220854Z",
     "shell.execute_reply": "2025-08-10T19:49:21.220247Z",
     "shell.execute_reply.started": "2025-08-10T19:49:16.939373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Here are some of the must‑see spots in London — a mix of iconic landmarks, world‑class museums, and vibrant neighborhoods:\n",
      "\n",
      "| # | Place | Why It’s Popular |\n",
      "|---|-------|------------------|\n",
      "| 1 | **Buckingham Palace** | The Queen’s official London residence – watch the Changing of the Guard. |\n",
      "| 2 | **The Tower of London & Tower Bridge** | Historic castle, Crown Jewels, and the iconic bridge with glass floors. |\n",
      "| 3 | **The British Museum** | World‑famous collection from the Rosetta Stone to Egyptian mummies (free entry). |\n",
      "| 4 | **The Houses of Parliament & Big Ben** | The classic symbol of London’s politics and architecture. |\n",
      "| 5 | **The National Gallery (Tate Britain)** | Home to masterpieces from Van Gogh to Turner. |\n",
      "| 6 | **Buckinghamshire Gardens (Kew Gardens)** | Stunning botanical gardens with a glasshouse and the Horniman Insect Zoo. |\n",
      "| 7 | **Camden Market** | Eclectic stalls, street food, music and vintage fashion. |\n",
      "| 8 | **Covent Garden** | Lively piazza with street performers, boutique shops, and the Royal Opera House. |\n",
      "| 9 | **West End Theatres** | Theatre district famous for grand productions (musicals, dramas). |\n",
      "|10 | **The Shard** | Skyscraper with panoramic 360° views of London. |\n",
      "|11 | **St. Paul’s Cathedral** | Massive dome, stunning interior and a climb up the Whispering Gallery. |\n",
      "|12 | **The Tate Modern** | Contemporary art museum set in a former power station. |\n",
      "|13 | **The Victoria & Albert Museum** | Design and fashion, costume, and jewelry collections. |\n",
      "|14 | **Hyde Park & Kensington Gardens** | Huge green spaces with Serpentine Lake, Speaker’s Corner and Speakers' Corner. |\n",
      "|15 | **Oxford Street & Regent Street** | Prime shopping streets for fashion, flagship stores, and historic architecture. |\n",
      "\n",
      "These spots cover history, culture, shopping, and leisure—perfect for a first visit or a weekend escape in London!\n",
      "-----\n",
      "\n",
      "{'prompt_tokens': 79, 'total_tokens': 619, 'completion_tokens': 540, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "    ],\n",
    "}\n",
    "res = llm.predict(payload)\n",
    "print(\"-----\\n\" + res[\"choices\"][0][\"message\"][\"content\"] + \"\\n-----\\n\")\n",
    "print(res[\"usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7830157c-db47-468b-b6e7-510546aaf703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T19:49:27.982505Z",
     "iopub.status.busy": "2025-08-10T19:49:27.982161Z",
     "iopub.status.idle": "2025-08-10T19:49:29.179651Z",
     "shell.execute_reply": "2025-08-10T19:49:29.179138Z",
     "shell.execute_reply.started": "2025-08-10T19:49:27.982482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "**9.8** is the larger number.\n",
      "\n",
      "- 9.11 is nine and eleven‑hundredths (9 + 0.11).  \n",
      "- 9.8 is nine and eight‑tenths (9 + 0.8).  \n",
      "\n",
      "Since 0.8 > 0.11, 9.8 > 9.11.\n",
      "-----\n",
      "\n",
      "{'prompt_tokens': 84, 'total_tokens': 233, 'completion_tokens': 149, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is bigger 9.11 or 9.8?\"}\n",
    "    ],\n",
    "}\n",
    "res = llm.predict(payload)\n",
    "print(\"-----\\n\" + res[\"choices\"][0][\"message\"][\"content\"] + \"\\n-----\\n\")\n",
    "print(res[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97335e2-65eb-4571-83dd-1cb7fae9bf9a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90af74a3-54f2-4576-8fac-9095ebb086cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T19:55:56.924769Z",
     "iopub.status.busy": "2025-08-10T19:55:56.924431Z",
     "iopub.status.idle": "2025-08-10T19:55:57.689309Z",
     "shell.execute_reply": "2025-08-10T19:55:57.688779Z",
     "shell.execute_reply.started": "2025-08-10T19:55:56.924747Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.delete_inference_component(inference_component_name)\n",
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9127a5-b04d-4d72-bcdb-8c03bc66d6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
